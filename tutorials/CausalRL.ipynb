{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Causal Reinforcement Learning with Frozen lake\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is an attempt at causal modeling of a reinforcement learning environment using pyro. Here we navigate the Open AI gym's Frozen lake environment that consists of discrete state and action space. \n",
    "\n",
    "*This is a work in progress*. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "from functools import partial\n",
    "from pyro import distributions as dist\n",
    "import pyro\n",
    "import torch\n",
    "import numpy as np\n",
    "import gym\n",
    "from copy import deepcopy\n",
    "from pyro.infer import Importance, EmpiricalMarginal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ACTIONS = [0, 1, 2, 3]\n",
    "\n",
    "# utility function to estimate the manhattan distance of a state\n",
    "def manhattan_distance(s, gridsize=4):\n",
    "    \"\"\" Get the manhattan distance from goal\"\"\"\n",
    "    # 2 dimensional vectors, s/4 gives row index , s%4 gives column index for a 4 * 4 grid\n",
    "    p = np.array([int(s / gridsize), (s % gridsize)])\n",
    "    q = np.array([gridsize, gridsize])\n",
    "    return sum(abs(p - q))\n",
    "\n",
    "\n",
    "def argmax(iterable, func):\n",
    "    \"\"\"Get the argmax of an iterable\"\"\"\n",
    "    return max(iterable, key=func)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sample an action given a state. Currently uses a uniform sampler irrespective of the state."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sample_action(state, i=0):\n",
    "    \"\"\" Uniform sampler of actions.  Ideally learned from data\"\"\"\n",
    "    probs = [1., 1., 1., 1.]\n",
    "    action = pyro.sample(f'action{state}{i}',\n",
    "                          dist.Categorical(torch.tensor(probs)))\n",
    "    return action"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The reward function here is modified to return a reward equal to the manhattan distance of the state from the goal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def reward(state, i=0):\n",
    "    \"\"\"Reward function given a state\"\"\"\n",
    "    # Goal is state 15, reward 1 point\n",
    "    if state == 15:\n",
    "        return pyro.sample(f'reward{state}{i}', dist.Delta(torch.tensor(1.)))\n",
    "    # Holes are state 5, 7, 11, 12\n",
    "    if state in [5, 7, 11, 12]:\n",
    "        return pyro.sample(f'reward{state}{i}', dist.Delta(torch.tensor(-10.)))\n",
    "    # Create a reward that grows as we get close to goal\n",
    "    r = 1 / float(manhattan_distance(state))\n",
    "    return pyro.sample(f'reward{state}{i}', dist.Delta(torch.tensor(r)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def expected_reward(Q_function, action, env, i):\n",
    "    def get_posterior_mean(posterior, n_samples=30):\n",
    "        \"\"\"\n",
    "        Calculate posterior mean\n",
    "        \"\"\"\n",
    "        # Sample\n",
    "        marginal_dist = EmpiricalMarginal(posterior).sample((n_samples, 1)).float()\n",
    "        # assumed to be all the same\n",
    "        return torch.mean(marginal_dist)\n",
    "    # The use of the param store is an optimization\n",
    "    param_name = 'posterior_reward_state{}_{}'.format(env.s, i)\n",
    "    if param_name in list(pyro.get_param_store().keys()):\n",
    "        posterior_mean = pyro.get_param_store().get_param(param_name)\n",
    "        return posterior_mean\n",
    "    else:\n",
    "        # this gets slower as we increase num_samples\n",
    "        inference = Importance(Q_function, num_samples=30)\n",
    "        posterior = inference.run(action, env, i)\n",
    "        posterior_mean = get_posterior_mean(posterior, 30)\n",
    "        pyro.param(param_name, posterior_mean)\n",
    "        return posterior_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def model(env, i=0):\n",
    "    \"\"\"Model of the environment\"\"\"\n",
    "    action = sample_action(env.s, i=i)\n",
    "    observation, reward, done, info = env.step(int(action))\n",
    "    return env, observation, reward, done, info\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We simulate a transition and observe the resulting environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def imagine_next_step(env, action, i):\n",
    "    \"\"\"Agent imagines next time step\"\"\"\n",
    "    sim_env = deepcopy(env)\n",
    "    state = sim_env.s\n",
    "    int_model = pyro.do(model, {f'action{state}{i}': action})\n",
    "    sim_env, _, _, _, _ = int_model(sim_env, i)\n",
    "    # sanity check\n",
    "    assert sim_env.lastaction == action\n",
    "    return sim_env"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q function that estimates the maximum expected reward for the current state and action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def Q(action, env, i):\n",
    "    \"\"\"Q function variant of Bellman equation.\"\"\"\n",
    "    utility = reward(env.s, i)\n",
    "    if utility not in [1.]:\n",
    "        env_step = imagine_next_step(env, action, i)\n",
    "    # check if the action got us closer to the goal. if yes only then recurse\n",
    "        if (env.s != env_step.s) and (reward(env_step.s) >= utility):\n",
    "            # Calculate expected rewards for each action but\n",
    "            # exclude backtracking actions.\n",
    "            expected_rewards = [\n",
    "                expected_reward(Q, act, env_step, i + 1)\n",
    "                for j, act in enumerate(ACTIONS)\n",
    "                if ACTIONS[abs(j - 2)] != action\n",
    "            ]\n",
    "            # Choose reward from optimal action\n",
    "            utility = utility + max(expected_rewards)\n",
    "\n",
    "    # pyro.factor(f'utility{i}', utility)\n",
    "    return utility"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Choose the action that maximizes the outcome of Q function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def policy(real_env, i=0):\n",
    "    # Choose optimal action\n",
    "    action = argmax(ACTIONS, partial(Q, env=real_env, i=i))\n",
    "    print(action)\n",
    "    return action\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run simulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "  (Down)\n",
      "SFFF\n",
      "\u001b[41mF\u001b[0mHFH\n",
      "FFFH\n",
      "HFFG\n",
      "1\n",
      "  (Down)\n",
      "SFFF\n",
      "FHFH\n",
      "\u001b[41mF\u001b[0mFFH\n",
      "HFFG\n",
      "2\n",
      "  (Right)\n",
      "SFFF\n",
      "FHFH\n",
      "F\u001b[41mF\u001b[0mFH\n",
      "HFFG\n",
      "2\n",
      "  (Right)\n",
      "SFFF\n",
      "FHFH\n",
      "FF\u001b[41mF\u001b[0mH\n",
      "HFFG\n",
      "1\n",
      "  (Down)\n",
      "SFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HF\u001b[41mF\u001b[0mG\n",
      "2\n",
      "  (Right)\n",
      "SFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFF\u001b[41mG\u001b[0m\n",
      "Episode finished after 6 timesteps\n"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    fl_env = gym.make('FrozenLake-v0', is_slippery=False)\n",
    "    fl_env.reset()\n",
    "    for t in range(25):\n",
    "        pyro.clear_param_store()\n",
    "        action = policy(fl_env)\n",
    "        int_model = pyro.do(model,\n",
    "                            {f'action{fl_env.s}{t}': torch.tensor(action)})\n",
    "        fl_env, observation, reward, done, info = int_model(fl_env, t)\n",
    "        fl_env.render()\n",
    "        if done:\n",
    "            print(\"Episode finished after {} timesteps\".format(t + 1))\n",
    "            break\n",
    "    fl_env.close()\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
