{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "from sklearn import datasets\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "import pyro\n",
    "from pyro.nn import PyroModule, PyroParam, PyroSample\n",
    "from pyro import poutine\n",
    "import pyro.distributions as dist\n",
    "import pyro.distributions.transforms as T\n",
    "from torch.optim import Adam\n",
    "\n",
    "import torch\n",
    "from torch import tensor, sigmoid\n",
    "from pyro.distributions import constraints\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "class Dozen(Dataset):\n",
    "    def __init__(self, n_samples, hole, noise=None, factor=None):\n",
    "        super().__init__()\n",
    "        self.data = self.make_dataset(n_samples, hole, noise, factor)\n",
    "\n",
    "    def make_dataset(self, n_samples, hole, noise=None, factor=None):\n",
    "        output = hole.model()\n",
    "        if not noise:\n",
    "            noise = output[0][1].item()\n",
    "        if not factor:\n",
    "            factor = output[1][1].item()\n",
    "        self.noise_scalar = noise\n",
    "        self.factor_scalar = factor\n",
    "        X, y = datasets.make_circles(n_samples=n_samples, noise=noise, factor=factor)\n",
    "        dataset = {'noise': (torch.ones([n_samples]).view(-1, 1)*noise).float(), \n",
    "                   'factor': (torch.ones([n_samples]).view(-1, 1)*factor).float(),\n",
    "                   'label': torch.tensor(y).view(-1, 1).float(),\n",
    "                   'x1': torch.tensor(X[:, 0]).view(-1, 1).float(),\n",
    "                   'x2': torch.tensor(X[:, 1]).view(-1, 1).float()}\n",
    "        return dataset\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return {name: val[idx] for name, val in self.data.items()}\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data.values())\n",
    "\n",
    "\n",
    "def plot(x1, x2, y):\n",
    "    X = StandardScaler().fit_transform(np.concatenate([x1, x2], axis=-1))\n",
    "\n",
    "    plt.title(r'Samples from $p(x_1,x_2)$')\n",
    "    plt.xlabel(r'$x_1$')\n",
    "    plt.ylabel(r'$x_2$')\n",
    "    plt.scatter(X[:,0], X[:,1], alpha=0.5, color=['red' if y_ == 1 else 'green' for y_ in y])\n",
    "    plt.show()\n",
    "\n",
    "    plt.subplot(1, 2, 1)\n",
    "    sns.distplot(X[:,0], hist=False, kde=True,\n",
    "                 bins=None,\n",
    "                 hist_kws={'edgecolor':'black'},\n",
    "                 kde_kws={'linewidth': 2})\n",
    "    plt.title(r'$p(x_1)$')\n",
    "    plt.subplot(1, 2, 2)\n",
    "    sns.distplot(X[:,1], hist=False, kde=True,\n",
    "                 bins=None,\n",
    "                 hist_kws={'edgecolor':'black'},\n",
    "                 kde_kws={'linewidth': 2})\n",
    "    plt.title(r'$p(x_2)$')\n",
    "    plt.show()\n",
    "\n",
    "def compare(model, dataset, n=1000):\n",
    "    x1, x2, y, factor, noise = sample_trained(model.plate_model, n)\n",
    "    X1, X2, Y, = dataset.data['x1'], dataset.data['x2'], dataset.data['label']\n",
    "\n",
    "    #plot(x1, x2, y)\n",
    "    plt.title(r'Joint Distribution')\n",
    "    plt.xlabel(r'$x_1$')\n",
    "    plt.ylabel(r'$x_2$')\n",
    "    plt.scatter(X1, X2, label='data', alpha=0.5, color=['red' if y_ == 1 else 'green' for y_ in Y])\n",
    "    plt.scatter(x1, x2, label='flow', alpha=0.5, color=['firebrick' if y_ == 1 else 'blue' for y_ in y])\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "    plt.subplot(1, 2, 1)\n",
    "    sns.distplot(X1, hist=False, kde=True, \n",
    "                 bins=None,\n",
    "                 hist_kws={'edgecolor':'black'},\n",
    "                 kde_kws={'linewidth': 2},\n",
    "                 label='data')\n",
    "    sns.distplot(x1, hist=False, kde=True, \n",
    "                 bins=None, color='firebrick',\n",
    "                 hist_kws={'edgecolor':'black'},\n",
    "                 kde_kws={'linewidth': 2},\n",
    "                 label='flow')\n",
    "    plt.title(r'$p(x_1)$')\n",
    "    plt.subplot(1, 2, 2)\n",
    "    sns.distplot(X1, hist=False, kde=True, \n",
    "                 bins=None,\n",
    "                 hist_kws={'edgecolor':'black'},\n",
    "                 kde_kws={'linewidth': 2},\n",
    "                 label='data')\n",
    "    sns.distplot(x1, hist=False, kde=True, \n",
    "                 bins=None, color='firebrick',\n",
    "                 hist_kws={'edgecolor':'black'},\n",
    "                 kde_kws={'linewidth': 2},\n",
    "                 label='flow')\n",
    "    plt.title(r'$p(x_2)$')\n",
    "    plt.show()\n",
    "\n",
    "def factor_mechanism(noise, N_factor):\n",
    "    return sigmoid(N_factor + noise)\n",
    "\n",
    "def sample_trained(plate_model, n=1000):\n",
    "    handler = poutine.trace(plate_model)\n",
    "    trace = handler.get_trace(n)\n",
    "    nodes = ['x1', 'x2', 'label', 'factor', 'noise']\n",
    "    x1, x2, y, factor, noise = [trace.nodes[node]['value'].detach().numpy() for node in nodes]\n",
    "    return x1, x2, y, factor, noise\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Donut(PyroModule):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.noise_alpha = pyro.param('noise_alpha', tensor(1.), constraint=constraints.positive)\n",
    "        self.noise_beta = pyro.param('noise_beta', tensor(1.), constraint=constraints.positive)\n",
    "        self.factor_loc = pyro.param('factor_loc', tensor(0.))\n",
    "        self.factor_scale = pyro.param('factor_scale', tensor(1.), constraint=constraints.positive)\n",
    "        self.label_prob = pyro.param('label_prob', tensor(0.5), constraint=constraints.positive)\n",
    "        self._build_flow()\n",
    "\n",
    "    def _build_flow(self):\n",
    "        self.x1_transform = T.conditional_spline(1, context_dim=3)\n",
    "        self.x2_transform = T.conditional_spline(1, context_dim=4)\n",
    "\n",
    "    def img_model(self):\n",
    "        # generate noise value\n",
    "        N_noise = pyro.sample('N_noise', dist.Beta(self.noise_alpha, self.noise_beta)) #.to_event(1))\n",
    "        #noise = pyro.sample('noise', dist.Delta(0.2 * N_noise)) #.to_event(1))\n",
    "\n",
    "        # generate factor value\n",
    "        N_factor = pyro.sample('N_factor', dist.Normal(self.factor_loc, self.factor_scale)) #.to_event(1))\n",
    "        factor = pyro.sample('factor', dist.Delta(factor_mechanism(noise, N_factor))) #.to_event(1))\n",
    "\n",
    "        # generate independent label value\n",
    "        N_label = pyro.sample('N_label', dist.Bernoulli(self.label_prob)) #.to_event(1))\n",
    "        label = pyro.sample('label', dist.Delta(N_label)) #.to_event(1))\n",
    "        #label.requires_grad = True\n",
    "\n",
    "        return (N_noise, 0.2 * N_noise), (N_factor, factor_mechanism(.2 * N_noise, N_factor)), (N_label, N_label)\n",
    "\n",
    "#     def img_model(self):\n",
    "#         # generate noise value\n",
    "#         N_noise = pyro.sample('N_noise', dist.Beta(self.noise_alpha, self.noise_beta)) #.to_event(1))\n",
    "#         noise = pyro.sample('noise', dist.Normal((0.2*N_noise), 1e-3)) #.to_event(1))\n",
    "\n",
    "#         # generate factor value\n",
    "#         N_factor = pyro.sample('N_factor', dist.Normal(self.factor_loc, self.factor_scale)) #.to_event(1))\n",
    "#         factor = pyro.sample('factor', dist.Normal(self.factor_mechanism(noise, N_factor), 1e-3)) #.to_event(1))\n",
    "\n",
    "#         # generate independent label value\n",
    "#         N_label = pyro.sample('N_label', dist.Bernoulli(self.label_prob)) #.to_event(1))\n",
    "#         label = pyro.sample('label', dist.Normal(N_label, 1e-3)) #.to_event(1))\n",
    "#         label.requires_grad = True\n",
    "\n",
    "#         return (N_noise, noise), (N_factor, factor), (N_label, label)\n",
    "\n",
    "    def model(self):\n",
    "        # generate values from the DAG for the image\n",
    "        (N_noise, noise), (N_factor, factor), (N_label, label) = self.img_model()\n",
    "\n",
    "        # x1 flow\n",
    "        base_dist = dist.Normal(torch.zeros(1), torch.ones(1)) #.to_event(1))\n",
    "        x1_context = torch.cat([noise.view(-1, 1), factor.view(-1, 1), label.view(-1, 1)], dim=-1).squeeze()\n",
    "        x1_context = torch.ones_like(x1_context).detach()\n",
    "        self.dist_x1_given_scm = dist.ConditionalTransformedDistribution(base_dist, [self.x1_transform])\n",
    "        x1 = pyro.sample('x1', self.dist_x1_given_scm.condition(x1_context).to_event(1))\n",
    "\n",
    "        cond_x1_transforms = T.ComposeTransform(self.dist_x1_given_scm.condition(x1_context).transforms)\n",
    "        N_x1 = cond_x1_transforms.inv(x1)\n",
    "\n",
    "        # x2 flow\n",
    "        self.dist_x2_given_scm = dist.ConditionalTransformedDistribution(base_dist, [self.x2_transform]) \n",
    "        x2_context = torch.cat([noise.view(-1, 1), factor.view(-1, 1), label.view(-1, 1), x1.view(-1, 1)], dim=-1).squeeze()\n",
    "        x2_context = torch.ones_like(x2_context).detach()\n",
    "\n",
    "        x2 = pyro.sample('x2', self.dist_x2_given_scm.condition(x2_context).to_event(1))\n",
    "        cond_x2_transforms = T.ComposeTransform(self.dist_x2_given_scm.condition(x2_context).transforms)\n",
    "        N_x2 = cond_x2_transforms.inv(x2)\n",
    "\n",
    "        return (N_noise, noise), (N_factor, factor), (N_label, label), (N_x1, x1), (N_x2, x2)\n",
    "\n",
    "    def plate_model(self, num_particles):\n",
    "        with pyro.plate('data', num_particles):\n",
    "            output = self.model()\n",
    "        return output\n",
    "\n",
    "    def get_logprobs(self, **obs):\n",
    "        _required_data = ('x1', 'x2', 'label', 'factor', 'noise')\n",
    "        assert set(obs.keys()) == set(_required_data), f'set(obs.keys()) = {set(obs.keys())}, '\n",
    "\n",
    "        cond_model = pyro.condition(self.plate_model, data=obs)\n",
    "        model_trace = poutine.trace(cond_model).get_trace(obs['x1'].shape[0])\n",
    "        model_trace.compute_log_prob()\n",
    "\n",
    "        # get the loss per site\n",
    "        log_probs = {}\n",
    "        for name, site in model_trace.nodes.items():\n",
    "            if site[\"type\"] == \"sample\" and site[\"is_observed\"]:\n",
    "                if site[\"fn\"] == \"Delta\":\n",
    "                    log_probs[name] = 0.\n",
    "                else:\n",
    "                    log_probs[name] = site[\"log_prob\"].mean()\n",
    "                #log_probs[name] = site[\"log_prob\"].mean()\n",
    "                log_prob_shape = site[\"log_prob\"].shape\n",
    "                value_shape = site[\"value\"].shape\n",
    "        return log_probs\n",
    "\n",
    "    def clear_cache(self):\n",
    "        try:\n",
    "            self.dist_x1_given_scm.clear_cache()\n",
    "            self.dist_x2_given_scm.clear_cache()\n",
    "        except:\n",
    "            pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/100 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** NameError: name 'pdb' is not defined\n",
      "None\n",
      "> \u001b[0;32m<ipython-input-47-7d41bd8c7df4>\u001b[0m(50)\u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m     48 \u001b[0;31m\u001b[0;31m#                 log_prob_shape = site[\"log_prob\"].shape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m     49 \u001b[0;31m\u001b[0;31m#                 value_shape = site[\"value\"].shape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m---> 50 \u001b[0;31m        \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlog_probs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m     51 \u001b[0;31m        \u001b[0mlosses\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m     52 \u001b[0;31m        \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mretain_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "ipdb>  log_probs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'noise': tensor(-inf), 'factor': tensor(-inf), 'label': tensor(-inf), 'x1': tensor(-1.1507, grad_fn=<MeanBackward0>), 'x2': tensor(-1.1946, grad_fn=<MeanBackward0>)}\n"
     ]
    }
   ],
   "source": [
    "pyro_model = Donut()\n",
    "dataset =  Dozen(n_samples=1000, hole=pyro_model, noise=0.01, factor=0.3)\n",
    "batch_size = 16\n",
    "optimizer = Adam(params=pyro_model.parameters())\n",
    "loader = DataLoader(dataset, batch_size=batch_size)\n",
    "num_epochs = 100\n",
    "\n",
    "pyro.clear_param_store()\n",
    "losses = list()\n",
    "for i in tqdm(range(num_epochs)):\n",
    "    for batch in loader:\n",
    "        optimizer.zero_grad()\n",
    "        log_probs = pyro_model.get_logprobs(**batch)\n",
    "        \n",
    "        cond_model = pyro.condition(pyro_model.plate_model, data=batch)\n",
    "        model_trace = poutine.trace(cond_model).get_trace(batch['x1'].shape[0])\n",
    "        model_trace.compute_log_prob()\n",
    "\n",
    "        # get the loss per site\n",
    "        log_probs = {}\n",
    "        for name, site in model_trace.nodes.items():\n",
    "            if site[\"type\"] == \"sample\" and site[\"is_observed\"]:\n",
    "                if site[\"fn\"] == \"Delta\":\n",
    "                    log_probs[name] = 0.\n",
    "                else:\n",
    "                    log_probs[name] = site[\"log_prob\"].mean()\n",
    "                #log_probs[name] = site[\"log_prob\"].mean()\n",
    "                log_prob_shape = site[\"log_prob\"].shape\n",
    "                value_shape = site[\"value\"].shape\n",
    "        breakpoint()\n",
    "\n",
    "        #breakpoint()\n",
    "#         cond_model = pyro.condition(plate_model(pyro_model, batch_size) , data=batch)\n",
    "#         model_trace = poutine.trace(cond_model).get_trace(batch_size)\n",
    "#         model_trace.compute_log_prob()\n",
    "\n",
    "#         # get the loss per site\n",
    "#         log_probs = {}\n",
    "#         nodes = model_trace.nodes.items()\n",
    "#         breakpoint()\n",
    "#         for name, site in nodes:\n",
    "#             if site[\"type\"] == \"sample\" and site[\"is_observed\"]:\n",
    "#                 if site[\"fn\"] == \"Delta\":\n",
    "#                     log_probs[name] = 0 # site['log_prob'].masked_fill(site['log_prob'] != 0, -0.5).mean()\n",
    "#                 else:\n",
    "#                     log_probs[name] = site[\"log_prob\"].mean()\n",
    "#                 #log_probs[name] = site[\"log_prob\"].mean()\n",
    "#                 log_prob_shape = site[\"log_prob\"].shape\n",
    "#                 value_shape = site[\"value\"].shape\n",
    "        loss = torch.stack(tuple(log_probs.values())).sum()\n",
    "        losses.append(loss.detach().item())\n",
    "        loss.backward(retain_graph=True)\n",
    "        optimizer.step()\n",
    "        pyro_model.clear_cache()\n",
    "\n",
    "compare(pyro_model, dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot(dozen.data['x1'], dozen.data['x2'], dozen.data['label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.distplot(factor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(coffee.losses)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# noise, factor = model()\n",
    "# X, y = datasets.make_circles(n_samples=10, noise=noise.item(), factor=factor.item())\n",
    "# x1, x2, y = dozen.data['x1'], dozen.data['x2'], dozen.data['label']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(f'noise: {noise.item()}')\n",
    "# print(f'factor: {factor.item()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
