{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is an overall tutorial of how to go about this project. The files can't actually be directly run in Jupyter Notebook since they require commandline arguments, but you can use the **sys** library and cell magic commands to execute the file while passing the commandline arguments as shown below. Be sure to go through the `requirements.txt` file to get all the libraries and repositories installed needed for this project to work."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Frozenlake implementations and Generalized MDPs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What we tried to achieve here:\n",
    "\n",
    "1. Planning as inference:\n",
    "Working pseudo-softmax agent capable of solving FrozenLake\n",
    "(with minimal reward shaping and no knowledge of the environment).\n",
    "Non-so-much-working other pseudo-softmax implementations.\n",
    "\n",
    "2. Generalization to other environments:\n",
    "Parsers for standard MDP and POMDP formats.\n",
    "PyroMDP & PyroPOMDP, OpenAI Gym environments which run as pyro probabilistic programs.\n",
    "Working softmax agent capable of solving `gridworld.mdp` environment.\n",
    "\n",
    "The goal here was to solve the problem of implementing a related type of agent, the softmax agent which evaluates its own policy to compute its policy. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Walking through the code for related `contol_as_inference.py`. As mentioned before the code chunks cannot be run directly here but would require a command-line like call which is shown below where we demonstrate a sample output.\n",
    "\n",
    "The code is distributed for 2 different implementations, one for FrozenLake environment and other for general MDPs, the breakdown would be shown below.\n",
    "\n",
    "The $main()$ function is used to provide choice of which implementation is to be run. It factors in the choice and generates the relative environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    assert args.policy in ('control-as-inference-like', 'softmax-like')\n",
    "\n",
    "    if args.policy == 'control-as-inference-like':\n",
    "        policy = policy_control_as_inference_like\n",
    "    elif args.policy == 'softmax-like':\n",
    "        policy = softmax_like\n",
    "\n",
    "    if args.mdp == 'frozenlake':\n",
    "        env = gym.make('FrozenLake-v0', is_slippery=False)\n",
    "        env = FrozenLakeWrapper(env)\n",
    "\n",
    "        trajectory_model = trajectory_model_frozenlake\n",
    "        agent_model = agent_models.get_agent_model('FrozenLake-v0')\n",
    "\n",
    "        # makes sure integer action is sent to frozenlake environment\n",
    "        def action_cast(action):\n",
    "            return action.item()\n",
    "\n",
    "    else:\n",
    "        env = make_mdp(args.mdp, episodic=True)\n",
    "        env = TimeLimit(env, 100)\n",
    "\n",
    "        trajectory_model = trajectory_model_mdp\n",
    "        agent_model = agent_models.get_agent_model(args.mdp)\n",
    "\n",
    "        # makes sure tensor action is sent to MDP environment\n",
    "        def action_cast(action):\n",
    "            return action\n",
    "\n",
    "    env.reset()\n",
    "    for t in itt.count():\n",
    "        print('---')\n",
    "        print(f't: {t}')\n",
    "        print('state:')\n",
    "        env.render()\n",
    "\n",
    "        action = policy(\n",
    "            env,\n",
    "            trajectory_model=trajectory_model,\n",
    "            agent_model=agent_model,\n",
    "            log=True,\n",
    "        )\n",
    "        _, reward, done, _ = env.step(action_cast(action))\n",
    "        print(f'reward: {reward}')\n",
    "\n",
    "        if done:\n",
    "            print('final state:')\n",
    "            env.render()\n",
    "            print(f'Episode finished after {t+1} timesteps')\n",
    "            break\n",
    "\n",
    "    env.close()\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument('mdp', help='`frozenlake` string or path to MDP file')\n",
    "    parser.add_argument(\n",
    "        '--policy',\n",
    "        choices=['control-as-inference-like', 'softmax-like'],\n",
    "        default='control-as-inference-like',\n",
    "        help='Choose one of two control strategies',\n",
    "    )\n",
    "    parser.add_argument('--alpha', type=float, default=100.0) # likelihood parameter\n",
    "    parser.add_argument('--gamma', type=float, default=0.95) # discount factor\n",
    "    parser.add_argument('--num-samples', type=int, default=2_000)\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    print(f'args: {args}')\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation to solve frozen lake environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We tried to create a softmax implementation FrozenLake but later realized that it was more similar to an implemtation of **control as inference**!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code is used to generate frozenlake environment trajectories. We use `pyro.factor()` is used to influence trace log-likelihood, it acts as soft-conditioning/filtering to select random trajectories which result in high sample return."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trajectory_model_frozenlake(env, *, agent_model=None, factor_G=False):\n",
    "    \"\"\"trajectory_model_frozenlake\n",
    "\n",
    "    A probabilistic program for the frozenlake environment trajectories.  The\n",
    "    sample return can be used to affect the trace likelihood.\n",
    "\n",
    "    :param env: OpenAI Gym FrozenLake environment\n",
    "    :param agent_model: agent's probabilistic program\n",
    "    :param factor_G: boolean; if True then apply $\\\\alpha G$ likelihood factor\n",
    "    \"\"\"\n",
    "    if agent_model is None:\n",
    "        agent_model = agent_models.uniform\n",
    "\n",
    "    env = deepcopy(env)\n",
    "\n",
    "    # running return and discount factor\n",
    "    return_, discount = 0.0, 1.0\n",
    "    for t in itt.count():\n",
    "        action = agent_model(f'A_{t}', env, env.s)\n",
    "        _, reward, done, _ = env.step(action.item())\n",
    "\n",
    "        # running return and discount factor\n",
    "        return_ += discount * reward\n",
    "        discount *= args.gamma\n",
    "\n",
    "        if done:\n",
    "            break\n",
    "\n",
    "    pyro.sample('G', Delta(torch.as_tensor(return_)))\n",
    "\n",
    "    if factor_G:\n",
    "        pyro.factor('factor_G', args.alpha * return_)\n",
    "\n",
    "    return return_\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The $policy\\_control\\_as\\_inference\\_like()$  function is used toapply importance sampling to sample action site $A_0$ and display the marginal probabilities in tabulated format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def policy_control_as_inference_like(\n",
    "    env, *, trajectory_model, agent_model, log=False\n",
    "):\n",
    "    \"\"\"policy_control_as_inference_like\n",
    "\n",
    "    Implements a control-as-inference-like policy which \"maximizes\"\n",
    "    $\\\\Pr(A_0 \\\\mid S_0, high G)$.\n",
    "\n",
    "    Not actually standard CaI, because we don't really condition on G;  rather,\n",
    "    we use $\\\\alpha G$ as a likelihood factor on sample traces.\n",
    "\n",
    "    :param env: OpenAI Gym environment\n",
    "    :param trajectory_model: trajectory probabilistic program\n",
    "    :param agent_model: agent's probabilistic program\n",
    "    :param log: boolean; if True, print log info\n",
    "    \"\"\"\n",
    "    inference = Importance(trajectory_model, num_samples=args.num_samples)\n",
    "    posterior = inference.run(env, agent_model=agent_model, factor_G=True)\n",
    "    marginal = EmpiricalMarginal(posterior, 'A_0')\n",
    "\n",
    "    if log:\n",
    "        samples = marginal.sample((args.num_samples,))\n",
    "        counts = Counter(samples.tolist())\n",
    "        hist = [counts[i] / args.num_samples for i in range(env.action_space.n)]\n",
    "        print('policy:')\n",
    "        print(tabulate([hist], headers=env.actions, tablefmt='fancy_grid'))\n",
    "\n",
    "    return marginal.sample()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation for General MDPs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a similar implementation of **control as inference** for the case of general MDP's. Before we were working on a predefined gym environment of Frozen lake but using the `make_mdp()` function in main, we make call to $PyroMDP$ implementation which is done in `gym-pyro` repository. This generated and returns a probailistic environment which can be used to solve by out agent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trajectory_model_mdp(env, *, agent_model=None, factor_G=False):\n",
    "    \"\"\"trajectory_model_mdp\n",
    "\n",
    "    A probabilistic program for MDP environment trajectories.  The sample return\n",
    "    can be used to affect the trace likelihood.\n",
    "\n",
    "    :param env: OpenAI Gym environment\n",
    "    :param agent_model: agent's probabilistic program\n",
    "    :param factor_G: boolean; if True then apply $\\\\alpha G$ likelihood factor\n",
    "    \"\"\"\n",
    "    if agent_model is None:\n",
    "        agent_model = agent_models.uniform\n",
    "\n",
    "    env = deepcopy(env)\n",
    "\n",
    "    # running return and discount factor\n",
    "    return_, discount = 0.0, 1.0\n",
    "\n",
    "    # with keep_state=True only the time-step used to name sites is being reset\n",
    "    state = env.reset(keep_state=True)\n",
    "    for t in itt.count():\n",
    "        action = agent_model(f'A_{t}', env, state)\n",
    "        state, reward, done, _ = env.step(action)\n",
    "\n",
    "        # running return and discount factor\n",
    "        return_ += discount * reward\n",
    "        discount *= args.gamma\n",
    "\n",
    "        if done:\n",
    "            break\n",
    "\n",
    "    pyro.sample('G', Delta(return_))\n",
    "\n",
    "    if factor_G:\n",
    "        pyro.factor('factor_G', args.alpha * return_)\n",
    "\n",
    "    return return_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function works similar to $policy\\_control\\_as\\_inference\\_like()$  function but for the general MDP case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax_like(env, *, trajectory_model, agent_model, log=False):\n",
    "    \"\"\"softmax_like\n",
    "\n",
    "    :param env: OpenAI Gym environment\n",
    "    :param trajectory_model: trajectory probabilistic program\n",
    "    :param agent_model: agent's probabilistic program\n",
    "    :param log: boolean; if True, print log info\n",
    "    \"\"\"\n",
    "\n",
    "    Qs = torch.as_tensor(\n",
    "        [\n",
    "            infer_Q(\n",
    "                env,\n",
    "                action,\n",
    "                trajectory_model=trajectory_model,\n",
    "                agent_model=agent_model,\n",
    "                log=log,\n",
    "            )\n",
    "            for action in range(env.action_space.n)\n",
    "        ]\n",
    "    )\n",
    "    action_logits = args.alpha * Qs\n",
    "    action_dist = Categorical(logits=action_logits)\n",
    "\n",
    "    if log:\n",
    "        print('policy:')\n",
    "        print(\n",
    "            tabulate(\n",
    "                [action_dist.probs.tolist()],\n",
    "                headers=env.actions,\n",
    "                tablefmt='fancy_grid',\n",
    "            )\n",
    "        )\n",
    "\n",
    "    return action_dist.sample()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sample execution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A sample output and demonstration of execution for the file `control_as_inference.py` taking `frozenlake` as an argument."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "args: Namespace(alpha=100.0, gamma=0.95, mdp='frozenlake', num_samples=2000, policy='control-as-inference-like')\n",
      "---\n",
      "t: 0\n",
      "state:\n",
      "\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "policy:\n",
      "╒════════╤════════╤═════════╤══════╕\n",
      "│   left │   down │   right │   up │\n",
      "╞════════╪════════╪═════════╪══════╡\n",
      "│      0 │      1 │       0 │    0 │\n",
      "╘════════╧════════╧═════════╧══════╛\n",
      "reward: 0.0\n",
      "---\n",
      "t: 1\n",
      "state:\n",
      "  (Down)\n",
      "SFFF\n",
      "\u001b[41mF\u001b[0mHFH\n",
      "FFFH\n",
      "HFFG\n",
      "policy:\n",
      "╒════════╤════════╤═════════╤════════╕\n",
      "│   left │   down │   right │     up │\n",
      "╞════════╪════════╪═════════╪════════╡\n",
      "│  0.005 │ 0.9945 │       0 │ 0.0005 │\n",
      "╘════════╧════════╧═════════╧════════╛\n",
      "reward: 0.0\n",
      "---\n",
      "t: 2\n",
      "state:\n",
      "  (Down)\n",
      "SFFF\n",
      "FHFH\n",
      "\u001b[41mF\u001b[0mFFH\n",
      "HFFG\n",
      "policy:\n",
      "╒════════╤════════╤═════════╤══════╕\n",
      "│   left │   down │   right │   up │\n",
      "╞════════╪════════╪═════════╪══════╡\n",
      "│ 0.0015 │      0 │  0.9985 │    0 │\n",
      "╘════════╧════════╧═════════╧══════╛\n",
      "reward: 0.0\n",
      "---\n",
      "t: 3\n",
      "state:\n",
      "  (Right)\n",
      "SFFF\n",
      "FHFH\n",
      "F\u001b[41mF\u001b[0mFH\n",
      "HFFG\n",
      "policy:\n",
      "╒════════╤════════╤═════════╤══════╕\n",
      "│   left │   down │   right │   up │\n",
      "╞════════╪════════╪═════════╪══════╡\n",
      "│      0 │  0.565 │   0.435 │    0 │\n",
      "╘════════╧════════╧═════════╧══════╛\n",
      "reward: 0.0\n",
      "---\n",
      "t: 4\n",
      "state:\n",
      "  (Down)\n",
      "SFFF\n",
      "FHFH\n",
      "FFFH\n",
      "H\u001b[41mF\u001b[0mFG\n",
      "policy:\n",
      "╒════════╤════════╤═════════╤══════╕\n",
      "│   left │   down │   right │   up │\n",
      "╞════════╪════════╪═════════╪══════╡\n",
      "│      0 │ 0.0045 │  0.9955 │    0 │\n",
      "╘════════╧════════╧═════════╧══════╛\n",
      "reward: 0.0\n",
      "---\n",
      "t: 5\n",
      "state:\n",
      "  (Right)\n",
      "SFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HF\u001b[41mF\u001b[0mG\n",
      "policy:\n",
      "╒════════╤════════╤═════════╤══════╕\n",
      "│   left │   down │   right │   up │\n",
      "╞════════╪════════╪═════════╪══════╡\n",
      "│      0 │  0.002 │   0.998 │    0 │\n",
      "╘════════╧════════╧═════════╧══════╛\n",
      "reward: 1.0\n",
      "final state:\n",
      "  (Right)\n",
      "SFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFF\u001b[41mG\u001b[0m\n",
      "Episode finished after 6 timesteps\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "%run control_as_inference.py \"frozenlake\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the output steps, we observe that out agent was able to solve optimally for the frozen lake environment in 6 timesteps to reach the goal."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Our final attempt at implementing Softmax"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Previously we tried to implement softmax but didn't quite succeed. In this attempt we think we reached the closest to implementing a real and efficient softmax.\n",
    "\n",
    "Here we have the implementation of 2 paths:\n",
    "1. Frozen lake: Frozen lake environment based on pyro importance sampling\n",
    "2. General MDP: We used `gridworld.mdp` to show implementation in general case where any environment can be defined in a `.mdp` format.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    env = make_mdp(args.mdp, episodic=True)\n",
    "    env = TimeLimit(env, 10)\n",
    "\n",
    "    env.reset()\n",
    "    for t in itt.count():\n",
    "        print('---')\n",
    "        print(f't: {t}')\n",
    "        print('state:')\n",
    "        env.render()\n",
    "\n",
    "        action = policy(env, log=True)\n",
    "        _, reward, done, _ = env.step(action)\n",
    "        print(f'reward: {reward}')\n",
    "\n",
    "        if done:\n",
    "            print('final state:')\n",
    "            env.render()\n",
    "            print(f'Episode finished after {t+1} timesteps')\n",
    "            break\n",
    "\n",
    "    env.close()\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument('mdp', help='path to MDP file')\n",
    "    parser.add_argument('--alpha', type=float, default=5_000.0)\n",
    "    parser.add_argument('--gamma', type=float, default=0.95)\n",
    "    parser.add_argument('--num-samples', type=int, default=20)\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    print(f'args: {args}')\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following function a probabilistic program for MDP environment trajectories using a presampled policy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trajectory_model(env, policy):\n",
    "    \"\"\"trajectory_model\n",
    "\n",
    "    A probabilistic program for MDP environment trajectories using a presampled\n",
    "    policy.\n",
    "\n",
    "    :param env: OpenAI Gym FrozenLake environment\n",
    "    :param policy: predetermined policy function\n",
    "    \"\"\"\n",
    "    env = deepcopy(env)\n",
    "\n",
    "    # running return and discount factor\n",
    "    return_, discount = 0.0, 1.0\n",
    "    for _ in itt.count():\n",
    "        action = policy(env.state)\n",
    "        _, reward, done, _ = env.step(action)\n",
    "\n",
    "        # running return and discount factor\n",
    "        return_ += discount * reward\n",
    "        discount *= args.gamma\n",
    "\n",
    "        if done:\n",
    "            break\n",
    "\n",
    "    return_ = pyro.sample(f'G', Delta(return_))\n",
    "\n",
    "    return return_\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following model is used to performs inference to estimate $Q^\\pi(s, a)$, then uses pyro.factor to modify the trace log-likelihood."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def softmax_agent_model(env):\n",
    "    \"\"\"softmax_agent_model\n",
    "\n",
    "    Softmax agent model;  Performs inference to estimate $Q^\\pi(s, a)$, then\n",
    "    uses pyro.factor to modify the trace log-likelihood.\n",
    "\n",
    "    :param env: OpenAI Gym environment\n",
    "    \"\"\"\n",
    "    policy_probs = torch.ones(env.state_space.n, env.action_space.n)\n",
    "    policy_vector = pyro.sample('policy_vector', Categorical(policy_probs))\n",
    "\n",
    "    inference = Importance(trajectory_model, num_samples=args.num_samples)\n",
    "    posterior = inference.run(env, lambda state: policy_vector[state])\n",
    "    Q = EmpiricalMarginal(posterior, 'G').mean\n",
    "\n",
    "    pyro.factor('factor_Q', args.alpha * Q)\n",
    "\n",
    "    return policy_vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We sample the policy using importance sampling on the entire above process. The action is chosen using the sample policy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def policy(env, log=False):\n",
    "    \"\"\"policy\n",
    "\n",
    "    :param env: OpenAI Gym environment\n",
    "    :param log: boolean; if True, print log info\n",
    "    \"\"\"\n",
    "    inference = Importance(softmax_agent_model, num_samples=args.num_samples)\n",
    "    posterior = inference.run(env)\n",
    "    marginal = EmpiricalMarginal(posterior, 'policy_vector')\n",
    "\n",
    "    if log:\n",
    "        policy_samples = marginal.sample((args.num_samples,))\n",
    "        action_samples = policy_samples[:, env.state]\n",
    "        counts = Counter(action_samples.tolist())\n",
    "        hist = [counts[i] / args.num_samples for i in range(env.action_space.n)]\n",
    "        print('policy:')\n",
    "        print(tabulate([hist], headers=env.actions, tablefmt='fancy_grid'))\n",
    "\n",
    "    policy_vector = marginal.sample()\n",
    "    return policy_vector[env.state]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We observed that it kinda works sometimes, but is very sensitive to hyper parameters. This was our final implementation of softmax, although we are not sure that its actually equivalent to the real softmax, a more formal proof is required for that which maybe a good idea for Future works on this."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sample execution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Following line of code shows a sample exectuion the code file for the `gridworld.mdp`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "args: Namespace(alpha=5000.0, gamma=0.95, mdp='gridworld.mdp', num_samples=20)\n",
      "---\n",
      "t: 0\n",
      "state:\n",
      "...+\n",
      ". .-\n",
      "\u001b[41m.\u001b[0m...\n",
      "policy:\n",
      "╒═════════╤═════════╤════════╤════════╕\n",
      "│   north │   south │   east │   west │\n",
      "╞═════════╪═════════╪════════╪════════╡\n",
      "│       0 │       0 │      0 │      1 │\n",
      "╘═════════╧═════════╧════════╧════════╛\n",
      "reward: -0.1\n",
      "---\n",
      "t: 1\n",
      "state:\n",
      "action: west\n",
      "...+\n",
      ". .-\n",
      "\u001b[41m.\u001b[0m...\n",
      "policy:\n",
      "╒═════════╤═════════╤════════╤════════╕\n",
      "│   north │   south │   east │   west │\n",
      "╞═════════╪═════════╪════════╪════════╡\n",
      "│       1 │       0 │      0 │      0 │\n",
      "╘═════════╧═════════╧════════╧════════╛\n",
      "reward: -0.1\n",
      "---\n",
      "t: 2\n",
      "state:\n",
      "action: north\n",
      "...+\n",
      "\u001b[41m.\u001b[0m .-\n",
      "....\n",
      "policy:\n",
      "╒═════════╤═════════╤════════╤════════╕\n",
      "│   north │   south │   east │   west │\n",
      "╞═════════╪═════════╪════════╪════════╡\n",
      "│       0 │       0 │      0 │      1 │\n",
      "╘═════════╧═════════╧════════╧════════╛\n",
      "reward: -0.1\n",
      "---\n",
      "t: 3\n",
      "state:\n",
      "action: west\n",
      "\u001b[41m.\u001b[0m..+\n",
      ". .-\n",
      "....\n",
      "policy:\n",
      "╒═════════╤═════════╤════════╤════════╕\n",
      "│   north │   south │   east │   west │\n",
      "╞═════════╪═════════╪════════╪════════╡\n",
      "│       0 │       0 │      1 │      0 │\n",
      "╘═════════╧═════════╧════════╧════════╛\n",
      "reward: -0.1\n",
      "---\n",
      "t: 4\n",
      "state:\n",
      "action: east\n",
      ".\u001b[41m.\u001b[0m.+\n",
      ". .-\n",
      "....\n",
      "policy:\n",
      "╒═════════╤═════════╤════════╤════════╕\n",
      "│   north │   south │   east │   west │\n",
      "╞═════════╪═════════╪════════╪════════╡\n",
      "│       0 │       0 │      1 │      0 │\n",
      "╘═════════╧═════════╧════════╧════════╛\n",
      "reward: -0.1\n",
      "---\n",
      "t: 5\n",
      "state:\n",
      "action: east\n",
      ".\u001b[41m.\u001b[0m.+\n",
      ". .-\n",
      "....\n",
      "policy:\n",
      "╒═════════╤═════════╤════════╤════════╕\n",
      "│   north │   south │   east │   west │\n",
      "╞═════════╪═════════╪════════╪════════╡\n",
      "│       0 │       0 │      1 │      0 │\n",
      "╘═════════╧═════════╧════════╧════════╛\n",
      "reward: -0.1\n",
      "---\n",
      "t: 6\n",
      "state:\n",
      "action: east\n",
      "..\u001b[41m.\u001b[0m+\n",
      ". .-\n",
      "....\n",
      "policy:\n",
      "╒═════════╤═════════╤════════╤════════╕\n",
      "│   north │   south │   east │   west │\n",
      "╞═════════╪═════════╪════════╪════════╡\n",
      "│       0 │       0 │      1 │      0 │\n",
      "╘═════════╧═════════╧════════╧════════╛\n",
      "reward: -0.1\n",
      "---\n",
      "t: 7\n",
      "state:\n",
      "action: east\n",
      "...\u001b[41m+\u001b[0m\n",
      ". .-\n",
      "....\n",
      "policy:\n",
      "╒═════════╤═════════╤════════╤════════╕\n",
      "│   north │   south │   east │   west │\n",
      "╞═════════╪═════════╪════════╪════════╡\n",
      "│    0.05 │     0.5 │   0.15 │    0.3 │\n",
      "╘═════════╧═════════╧════════╧════════╛\n",
      "reward: 1.0\n",
      "final state:\n",
      "action: east\n",
      "...+\n",
      ". .-\n",
      "\u001b[41m.\u001b[0m...\n",
      "Episode finished after 8 timesteps\n"
     ]
    }
   ],
   "source": [
    "%run softmax_presample_policy.py gridworld.mdp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that the agent was able to solve the problem in 8 timesteps and gain the final reward of $1$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Observing effect of Confounding on our agent's ability to solve the environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The main goal here was to observe the effect of confounding by observing for conditioning vs intervening on action for a general RL problem using a Confounding MDPs file which can be thought as a special case of partially observable MDPs (POMDPs).\n",
    "\n",
    "$E[R_t | S_t = s, do(A_t = a)] \\neq E[R_t | S_t = s, A_t = a]$\n",
    "\n",
    "We make use of the OpenAI gym environment framework and use a special format cmdp file. The cmdp file is derived from the encoding format for a MDP/POMDP problem, containing the states, rewards, confounders, actions and transition probabilities. The cmdp file is parsed into the environment using specially created parser from rl_parser repository which translates this file to a problem in gym environment which can be solved using traditional reinforcement learning techniques.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code for this part is implemented in `confounding_mdp.py` file.\n",
    "\n",
    "The following code is dependent on `PyroCMDP` implemetation from `gym-pyro` repository.\n",
    "\n",
    "The following code is used to make a call to the Pyro file which creates samples and generates the confounding environment for the agent to solve.\n",
    "\n",
    "`from envs import make_cmdp`\n",
    "\n",
    "Here we are using circles.cmpd which is nothing but a 3 x 3 grid environment with a binary confounder. The counfounders here are Clockwise and Counterclockwise direction enforcers. The agent receives positive reward for moving alongside the border depending on the confounder.\n",
    "\n",
    "The $main()$ function is where all the function calls are made. The $Qs$ values are calculated by making recurring calls to the $infer\\_Q()$ function and are displayed in a tabulated format.\n",
    "\n",
    "We let the agent work for 10 timesteps and observed the effects of confounders on expected value conditioning on agents action vs expected value by making agent do A action. The agent model always tries to pick optimal action based on the intervention distribution. The confounding effect becomes more apparent since the agent model behaves differently according to it.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def main():\n",
    "    env = make_cmdp(args.cmdp, episodic=True)\n",
    "    env = TimeLimit(env, 10)\n",
    "\n",
    "    agent_model_name = args.cmdp.split('/')[-1]\n",
    "    agent_model = agent_models.get_agent_model(agent_model_name)\n",
    "\n",
    "    values_df_index = 'E[G]', 'E[G | A=a]', 'E[G | do(A=a)]'\n",
    "    values_df_columns = env.model.actions\n",
    "\n",
    "    _, state = env.reset()\n",
    "    for t in itt.count():\n",
    "        print()\n",
    "        print(f't: {t}')\n",
    "        env.render()\n",
    "\n",
    "        Qs_none = [\n",
    "            infer_Q(env, action, 'none', agent_model=agent_model).item()\n",
    "            for action in range(env.action_space.n)\n",
    "        ]\n",
    "        Qs_condition = [\n",
    "            infer_Q(env, action, 'condition', agent_model=agent_model).item()\n",
    "            for action in range(env.action_space.n)\n",
    "        ]\n",
    "        Qs_intervention = [\n",
    "            infer_Q(env, action, 'intervention', agent_model=agent_model).item()\n",
    "            for action in range(env.action_space.n)\n",
    "        ]\n",
    "\n",
    "        values_df = pd.DataFrame(\n",
    "            [Qs_none, Qs_condition, Qs_intervention],\n",
    "            values_df_index,\n",
    "            values_df_columns,\n",
    "        )\n",
    "        print(values_df)\n",
    "\n",
    "        action = torch.tensor(Qs_intervention).argmax()\n",
    "        state, _, done, _ = env.step(action)\n",
    "\n",
    "        if done:\n",
    "            print()\n",
    "            print(f'final state: {state}')\n",
    "            print(f'Episode finished after {t+1} timesteps')\n",
    "            break\n",
    "\n",
    "    env.close()\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument('cmdp', help='CMDP file')\n",
    "    parser.add_argument(\n",
    "        '--gamma', type=float, default=0.95, help='discount factor'\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        '--num-samples',\n",
    "        type=int,\n",
    "        default=1_000,\n",
    "        help='number of samples to be used for importance sampling',\n",
    "    )\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    print(f'args: {args}')\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The trajectory function is used to simulate a trajectory by sampling random actions. The parameters here as the Open AI gym environment and the agent's probabilistic program"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trajectory_model(env, *, agent_model):\n",
    "    \"\"\"trajectory_model\n",
    "\n",
    "    A probabilistic program which simulates a trajectory by sampling random\n",
    "    actions.  The sample return can be used to affect the trace likelihood such\n",
    "    that the agent policy becomes\n",
    "\n",
    "    $\\\\pi(action_0; state_0) \\\\propto \\\\exp(\\\\alpha return_0)$\n",
    "\n",
    "    :param env: OpenAI Gym environment\n",
    "    :param agent_model: agent's probabilistic program\n",
    "    \"\"\"\n",
    "    env = deepcopy(env)\n",
    "\n",
    "    # initializing the running return and discount factor\n",
    "    return_, discount = 0.0, 1.0\n",
    "\n",
    "    # with keep_state=True only the time-step used to name sites is being reset\n",
    "    state, confounder = env.reset(keep_state=True)\n",
    "    for t in itt.count():\n",
    "        action = agent_model(f'A_{t}', env, (state, confounder)) #agent model\n",
    "        state, reward, done, _ = env.step(action) #environment model\n",
    "\n",
    "        # updating the running return and discount factor\n",
    "        return_ += discount * reward\n",
    "        discount *= args.gamma\n",
    "\n",
    "        if done:\n",
    "            break\n",
    "\n",
    "    pyro.sample('G', Delta(return_))\n",
    "\n",
    "    return return_\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The infer_Q function here is used to get posteriors for intervention and conditioning for every action A in working action space. We show the calculated effects in the tables displayed. G here stands for goal and A for action\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def infer_Q(env, action, infer_type='intervention', *, agent_model):\n",
    "    \"\"\"infer_Q\n",
    "\n",
    "    Infer Q(state, action) via pyro's importance sampling, via conditioning or\n",
    "    intervention.\n",
    "\n",
    "    :param env: OpenAI Gym environment\n",
    "    :param action: integer action\n",
    "    :param infer_type: type of inference; none, condition, or intervention\n",
    "    :param agent_model: agent's probabilistic program\n",
    "    \"\"\"\n",
    "    if infer_type not in ('intervention', 'condition', 'none'):\n",
    "        raise ValueError('Invalid inference type {infer_type}')\n",
    "\n",
    "    if infer_type == 'intervention':\n",
    "        model = pyro.do(trajectory_model, {'A_0': torch.tensor(action)})\n",
    "    elif infer_type == 'condition':\n",
    "        model = pyro.condition(trajectory_model, {'A_0': torch.tensor(action)})\n",
    "    else:  # infer_type == 'none'\n",
    "        model = trajectory_model\n",
    "\n",
    "    posterior = Importance(model, num_samples=args.num_samples).run(\n",
    "        env, agent_model=agent_model\n",
    "    )\n",
    "    return EmpiricalMarginal(posterior, 'G').mean\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sample execution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A sample output and demonstration of execution for the file `confounding_mdp.py` ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "args: Namespace(cmdp='circle.cmdp', gamma=0.95, num_samples=1000)\n",
      "\n",
      "t: 0\n",
      "...\n",
      "...\n",
      "..\u001b[41m.\u001b[0m\n",
      "                      up      down      left     right\n",
      "E[G]            8.025261  8.025261  8.025261  8.025261\n",
      "E[G | A=a]      8.025261  7.025261  8.025261  7.025261\n",
      "E[G | do(A=a)]  7.522261  7.025261  7.536261  7.025261\n",
      "\n",
      "t: 1\n",
      "action: left\n",
      "...\n",
      "...\n",
      ".\u001b[41m.\u001b[0m.\n",
      "                      up      down      left     right\n",
      "E[G]            8.025261  8.025261  8.025261  8.025261\n",
      "E[G | A=a]      6.075261  7.025261  8.025261  8.025261\n",
      "E[G | do(A=a)]  6.075261  7.025261  7.527261  7.542261\n",
      "\n",
      "t: 2\n",
      "action: right\n",
      "...\n",
      "...\n",
      "..\u001b[41m.\u001b[0m\n",
      "                      up      down      left     right\n",
      "E[G]            8.025261  8.025261  8.025261  8.025261\n",
      "E[G | A=a]      8.025261  7.025261  8.025261  7.025261\n",
      "E[G | do(A=a)]  7.560261  7.025261  7.507261  7.025261\n",
      "\n",
      "t: 3\n",
      "action: up\n",
      "...\n",
      "..\u001b[41m.\u001b[0m\n",
      "...\n",
      "                      up      down      left     right\n",
      "E[G]            8.025261  8.025261  8.025261  8.025261\n",
      "E[G | A=a]      8.025261  8.025261  6.075261  7.025261\n",
      "E[G | do(A=a)]  7.513261  7.497261  6.075261  7.025261\n",
      "\n",
      "t: 4\n",
      "action: up\n",
      "..\u001b[41m.\u001b[0m\n",
      "...\n",
      "...\n",
      "                      up      down      left     right\n",
      "E[G]            8.025261  8.025261  8.025261  8.025261\n",
      "E[G | A=a]      7.025261  8.025261  8.025261  7.025261\n",
      "E[G | do(A=a)]  7.025261  7.515261  7.505261  7.025261\n",
      "\n",
      "t: 5\n",
      "action: down\n",
      "...\n",
      "..\u001b[41m.\u001b[0m\n",
      "...\n",
      "                      up      down      left     right\n",
      "E[G]            8.025261  8.025261  8.025261  8.025261\n",
      "E[G | A=a]      8.025261  8.025261  6.075261  7.025261\n",
      "E[G | do(A=a)]  7.546261  7.506261  6.075261  7.025261\n",
      "\n",
      "t: 6\n",
      "action: up\n",
      "..\u001b[41m.\u001b[0m\n",
      "...\n",
      "...\n",
      "                      up      down      left     right\n",
      "E[G]            8.025261  8.025261  8.025261  8.025261\n",
      "E[G | A=a]      7.025261  8.025261  8.025261  7.025261\n",
      "E[G | do(A=a)]  7.025261  7.502261  7.522261  7.025261\n",
      "\n",
      "t: 7\n",
      "action: left\n",
      ".\u001b[41m.\u001b[0m.\n",
      "...\n",
      "...\n",
      "                      up      down      left     right\n",
      "E[G]            8.025261  8.025261  8.025261  8.025261\n",
      "E[G | A=a]      7.025261  6.075261  8.025261  8.025261\n",
      "E[G | do(A=a)]  7.025261  6.075261  7.518261  7.504261\n",
      "\n",
      "t: 8\n",
      "action: left\n",
      "\u001b[41m.\u001b[0m..\n",
      "...\n",
      "...\n",
      "                      up      down      left     right\n",
      "E[G]            8.025261  8.025261  8.025261  8.025261\n",
      "E[G | A=a]      7.025261  8.025261  7.025261  8.025261\n",
      "E[G | do(A=a)]  7.025261  7.494261  7.025261  7.504261\n",
      "\n",
      "t: 9\n",
      "action: right\n",
      ".\u001b[41m.\u001b[0m.\n",
      "...\n",
      "...\n",
      "                      up      down      left     right\n",
      "E[G]            8.025261  8.025261  8.025261  8.025261\n",
      "E[G | A=a]      7.025261  6.075261  8.025261  8.025261\n",
      "E[G | do(A=a)]  7.025261  6.075261  7.533261  7.504261\n",
      "\n",
      "final state: 0\n",
      "Episode finished after 10 timesteps\n"
     ]
    }
   ],
   "source": [
    "%run confounding_mdp.py circle.cmdp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "E[G] here is the expectation of reaching goal when agent model is choosing the action by itself. The agent model always chooses the same optimal action.\n",
    "\n",
    "E[G | A=a] here represents the expectation of reaching goal while observing for an action A=a in the action space.\n",
    "\n",
    "E[G| do(A=a)] here is used to represent the expectation of reaching goal when we intervene and set the action A=a.\n",
    "\n",
    "From the results we try to observe that conditioning and intervening give us two different values. To understand the results lets focus on say t: 8.\n",
    "\n",
    "So here we observe that the agent just moved to left after the previous time step reaches state s00. Computing the expectations for the possible action space now, comparing the conditioning and do action on A=a we observe that the expectation for conditioning and intervening stays the same for up and left since those actions are not permitted as s00 represents the top left of the grid. Although when we see for the possible actions which are down and right it seems that conditioning overestimate the values for moving down and right whereas do operation shows that the values should be lower than what are being expected. This could show an observed effect of confounding which appears while conditioning. Moving to the time step 9 we see that the agent actually moved to right and the reward changed to 0.0 which shows that it was not the best action to take maybe it could have had a better reward if it moved down instead. At the end of 10 timesteps we see that the agent end at 0 as the final state.\n",
    "\n",
    "Hence, we were able to explore the difference between “conditional” RL and causal RL and this concluded our presentation for the project."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
