{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "V100"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Chapter 13 - Causality and Large Language Models\n",
        "\n",
        "The notebook is a code companion to chapter 13 of the book [Causal AI](https://www.manning.com/books/causal-ai) by [Robert Osazuwa Ness](https://www.linkedin.com/in/osazuwa/).\n",
        "\n",
        "<a href=\"https://colab.research.google.com/github/altdeep/causalML/blob/master/book/chapter%2013/Chapter_13_Causality_LLMs.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ],
      "metadata": {
        "id": "DXRiRkWQeDWP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This notebook was written with the following libraries and versions."
      ],
      "metadata": {
        "id": "oQbJMbSrejuF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install dowhy==0.11\n",
        "!pip install transformers==4.38.2\n",
        "!pip install accelerate==0.28.0\n",
        "!pip install pandas==2.0.3\n",
        "!pip install numpy==1.25.2\n",
        "!pip install pyro-ppl==1.9.0"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MTvs6zwq_zMm",
        "outputId": "dfaa7ab6-4bcf-4843-f4e1-52e5644a6f53"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: dowhy==0.11 in /usr/local/lib/python3.10/dist-packages (0.11)\n",
            "Requirement already satisfied: causal-learn>=0.1.3.0 in /usr/local/lib/python3.10/dist-packages (from dowhy==0.11) (0.1.3.8)\n",
            "Requirement already satisfied: cvxpy<2.0.0,>=1.2.2 in /usr/local/lib/python3.10/dist-packages (from dowhy==0.11) (1.5.2)\n",
            "Requirement already satisfied: cython>=0.29.32 in /usr/local/lib/python3.10/dist-packages (from dowhy==0.11) (3.0.10)\n",
            "Requirement already satisfied: joblib>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from dowhy==0.11) (1.4.2)\n",
            "Requirement already satisfied: networkx>=2.8.5 in /usr/local/lib/python3.10/dist-packages (from dowhy==0.11) (3.3)\n",
            "Requirement already satisfied: numpy>=1.20 in /usr/local/lib/python3.10/dist-packages (from dowhy==0.11) (1.25.2)\n",
            "Requirement already satisfied: pandas>=1.4.3 in /usr/local/lib/python3.10/dist-packages (from dowhy==0.11) (2.0.3)\n",
            "Requirement already satisfied: scikit-learn>1.0 in /usr/local/lib/python3.10/dist-packages (from dowhy==0.11) (1.3.2)\n",
            "Requirement already satisfied: scipy>=1.4.1 in /usr/local/lib/python3.10/dist-packages (from dowhy==0.11) (1.13.1)\n",
            "Requirement already satisfied: statsmodels>=0.13.5 in /usr/local/lib/python3.10/dist-packages (from dowhy==0.11) (0.14.2)\n",
            "Requirement already satisfied: sympy>=1.10.1 in /usr/local/lib/python3.10/dist-packages (from dowhy==0.11) (1.13.1)\n",
            "Requirement already satisfied: tqdm>=4.64.0 in /usr/local/lib/python3.10/dist-packages (from dowhy==0.11) (4.66.4)\n",
            "Requirement already satisfied: graphviz in /usr/local/lib/python3.10/dist-packages (from causal-learn>=0.1.3.0->dowhy==0.11) (0.20.3)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (from causal-learn>=0.1.3.0->dowhy==0.11) (3.7.1)\n",
            "Requirement already satisfied: pydot in /usr/local/lib/python3.10/dist-packages (from causal-learn>=0.1.3.0->dowhy==0.11) (1.4.2)\n",
            "Requirement already satisfied: osqp>=0.6.2 in /usr/local/lib/python3.10/dist-packages (from cvxpy<2.0.0,>=1.2.2->dowhy==0.11) (0.6.7.post0)\n",
            "Requirement already satisfied: ecos>=2 in /usr/local/lib/python3.10/dist-packages (from cvxpy<2.0.0,>=1.2.2->dowhy==0.11) (2.0.14)\n",
            "Requirement already satisfied: clarabel>=0.5.0 in /usr/local/lib/python3.10/dist-packages (from cvxpy<2.0.0,>=1.2.2->dowhy==0.11) (0.9.0)\n",
            "Requirement already satisfied: scs>=3.2.4.post1 in /usr/local/lib/python3.10/dist-packages (from cvxpy<2.0.0,>=1.2.2->dowhy==0.11) (3.2.6)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.4.3->dowhy==0.11) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.4.3->dowhy==0.11) (2024.1)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.4.3->dowhy==0.11) (2024.1)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>1.0->dowhy==0.11) (3.5.0)\n",
            "Requirement already satisfied: patsy>=0.5.6 in /usr/local/lib/python3.10/dist-packages (from statsmodels>=0.13.5->dowhy==0.11) (0.5.6)\n",
            "Requirement already satisfied: packaging>=21.3 in /usr/local/lib/python3.10/dist-packages (from statsmodels>=0.13.5->dowhy==0.11) (24.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy>=1.10.1->dowhy==0.11) (1.3.0)\n",
            "Requirement already satisfied: qdldl in /usr/local/lib/python3.10/dist-packages (from osqp>=0.6.2->cvxpy<2.0.0,>=1.2.2->dowhy==0.11) (0.1.7.post4)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from patsy>=0.5.6->statsmodels>=0.13.5->dowhy==0.11) (1.16.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->causal-learn>=0.1.3.0->dowhy==0.11) (1.2.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib->causal-learn>=0.1.3.0->dowhy==0.11) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->causal-learn>=0.1.3.0->dowhy==0.11) (4.53.1)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->causal-learn>=0.1.3.0->dowhy==0.11) (1.4.5)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->causal-learn>=0.1.3.0->dowhy==0.11) (9.4.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->causal-learn>=0.1.3.0->dowhy==0.11) (3.1.2)\n",
            "Requirement already satisfied: transformers==4.38.2 in /usr/local/lib/python3.10/dist-packages (4.38.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers==4.38.2) (3.15.4)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.19.3 in /usr/local/lib/python3.10/dist-packages (from transformers==4.38.2) (0.23.5)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers==4.38.2) (1.25.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers==4.38.2) (24.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers==4.38.2) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers==4.38.2) (2024.5.15)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers==4.38.2) (2.31.0)\n",
            "Requirement already satisfied: tokenizers<0.19,>=0.14 in /usr/local/lib/python3.10/dist-packages (from transformers==4.38.2) (0.15.2)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers==4.38.2) (0.4.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers==4.38.2) (4.66.4)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.19.3->transformers==4.38.2) (2024.6.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.19.3->transformers==4.38.2) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.38.2) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.38.2) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.38.2) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.38.2) (2024.7.4)\n",
            "Requirement already satisfied: accelerate==0.28.0 in /usr/local/lib/python3.10/dist-packages (0.28.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from accelerate==0.28.0) (1.25.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from accelerate==0.28.0) (24.1)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate==0.28.0) (5.9.5)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from accelerate==0.28.0) (6.0.1)\n",
            "Requirement already satisfied: torch>=1.10.0 in /usr/local/lib/python3.10/dist-packages (from accelerate==0.28.0) (2.3.1+cu121)\n",
            "Requirement already satisfied: huggingface-hub in /usr/local/lib/python3.10/dist-packages (from accelerate==0.28.0) (0.23.5)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from accelerate==0.28.0) (0.4.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate==0.28.0) (3.15.4)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate==0.28.0) (4.12.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate==0.28.0) (1.13.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate==0.28.0) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate==0.28.0) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate==0.28.0) (2024.6.1)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate==0.28.0) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate==0.28.0) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate==0.28.0) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate==0.28.0) (8.9.2.26)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate==0.28.0) (12.1.3.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate==0.28.0) (11.0.2.54)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate==0.28.0) (10.3.2.106)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate==0.28.0) (11.4.5.107)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate==0.28.0) (12.1.0.106)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.20.5 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate==0.28.0) (2.20.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate==0.28.0) (12.1.105)\n",
            "Requirement already satisfied: triton==2.3.1 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate==0.28.0) (2.3.1)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.10/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.10.0->accelerate==0.28.0) (12.5.82)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->accelerate==0.28.0) (2.31.0)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->accelerate==0.28.0) (4.66.4)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.10.0->accelerate==0.28.0) (2.1.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate==0.28.0) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate==0.28.0) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate==0.28.0) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate==0.28.0) (2024.7.4)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.10.0->accelerate==0.28.0) (1.3.0)\n",
            "Requirement already satisfied: pandas==2.0.3 in /usr/local/lib/python3.10/dist-packages (2.0.3)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas==2.0.3) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas==2.0.3) (2024.1)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas==2.0.3) (2024.1)\n",
            "Requirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.10/dist-packages (from pandas==2.0.3) (1.25.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas==2.0.3) (1.16.0)\n",
            "Requirement already satisfied: numpy==1.25.2 in /usr/local/lib/python3.10/dist-packages (1.25.2)\n",
            "Requirement already satisfied: pyro-ppl==1.9.0 in /usr/local/lib/python3.10/dist-packages (1.9.0)\n",
            "Requirement already satisfied: numpy>=1.7 in /usr/local/lib/python3.10/dist-packages (from pyro-ppl==1.9.0) (1.25.2)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from pyro-ppl==1.9.0) (3.3.0)\n",
            "Requirement already satisfied: pyro-api>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from pyro-ppl==1.9.0) (0.1.2)\n",
            "Requirement already satisfied: torch>=2.0 in /usr/local/lib/python3.10/dist-packages (from pyro-ppl==1.9.0) (2.3.1+cu121)\n",
            "Requirement already satisfied: tqdm>=4.36 in /usr/local/lib/python3.10/dist-packages (from pyro-ppl==1.9.0) (4.66.4)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=2.0->pyro-ppl==1.9.0) (3.15.4)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch>=2.0->pyro-ppl==1.9.0) (4.12.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=2.0->pyro-ppl==1.9.0) (1.13.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=2.0->pyro-ppl==1.9.0) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=2.0->pyro-ppl==1.9.0) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=2.0->pyro-ppl==1.9.0) (2024.6.1)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=2.0->pyro-ppl==1.9.0) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=2.0->pyro-ppl==1.9.0) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=2.0->pyro-ppl==1.9.0) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /usr/local/lib/python3.10/dist-packages (from torch>=2.0->pyro-ppl==1.9.0) (8.9.2.26)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.10/dist-packages (from torch>=2.0->pyro-ppl==1.9.0) (12.1.3.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.10/dist-packages (from torch>=2.0->pyro-ppl==1.9.0) (11.0.2.54)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.10/dist-packages (from torch>=2.0->pyro-ppl==1.9.0) (10.3.2.106)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.10/dist-packages (from torch>=2.0->pyro-ppl==1.9.0) (11.4.5.107)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.10/dist-packages (from torch>=2.0->pyro-ppl==1.9.0) (12.1.0.106)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.20.5 in /usr/local/lib/python3.10/dist-packages (from torch>=2.0->pyro-ppl==1.9.0) (2.20.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=2.0->pyro-ppl==1.9.0) (12.1.105)\n",
            "Requirement already satisfied: triton==2.3.1 in /usr/local/lib/python3.10/dist-packages (from torch>=2.0->pyro-ppl==1.9.0) (2.3.1)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.10/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch>=2.0->pyro-ppl==1.9.0) (12.5.82)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=2.0->pyro-ppl==1.9.0) (2.1.5)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=2.0->pyro-ppl==1.9.0) (1.3.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Listing 13.1: DoWhy code generated by ChatGPT (verbatim)\n",
        "\n",
        "Modern LLMs are trained on a vast corpora of code from a wide variety of programming languages. You can ask the LLM to generate code from libraries that are represented in the training data. For example, we can prompt ChatGPT to implement a lung cancer DAG in DoWhy and estimate the causal effect.\n",
        "\n",
        "> **User**: Provide me with code that uses the DoWhy library in Python to implement the causal DAG that you suggested above. Include cigarette price as an instrumental variable and provide code for estimating the causal effect.\n",
        "\n",
        "> **ChatGPT**: Certainly! Here’s a basic example using the DoWhy library in Python for implementing the causal DAG for the relationship between smoking and lung cancer, using cigarette price as an instrumental variable.\n",
        "\n",
        "This is the verbatim code ChatGPT generated."
      ],
      "metadata": {
        "id": "C1MNO1jYSFMD"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3Fs95nOq_wu4",
        "outputId": "ee34e062-f0b4-49b2-c654-b61a90965e27"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:dowhy.causal_model:Causal Graph not provided. DoWhy will construct a graph based on data inputs.\n",
            "/usr/local/lib/python3.10/dist-packages/dowhy/causal_model.py:559: UserWarning: 4 variables are assumed unobserved because they are not in the dataset. Configure the logging level to `logging.WARNING` or higher for additional details.\n",
            "  warnings.warn(\n",
            "WARNING:dowhy.causal_model:The graph defines 7 variables. 3 were found in the dataset and will be analyzed as observed variables. 4 were not found in the dataset and will be analyzed as unobserved variables. The observed variables are: '['LC', 'Price', 'S']'. The unobserved variables are: '['A', 'E', 'G', 'O']'. If this matches your expectations for observations, please continue. If you expected any of the unobserved variables to be in the dataframe, please check for typos.\n",
            "WARNING:dowhy.causal_identifier.auto_identifier:Backdoor identification failed.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "*** Causal Estimate ***\n",
            "\n",
            "## Identified estimand\n",
            "Estimand type: EstimandType.NONPARAMETRIC_ATE\n",
            "\n",
            "### Estimand : 1\n",
            "Estimand name: iv\n",
            "Estimand expression:\n",
            " ⎡                            -1⎤\n",
            " ⎢   d         ⎛   d         ⎞  ⎥\n",
            "E⎢────────(LC)⋅⎜────────([S])⎟  ⎥\n",
            " ⎣d[Price]     ⎝d[Price]     ⎠  ⎦\n",
            "Estimand assumption 1, As-if-random: If U→→LC then ¬(U →→{Price})\n",
            "Estimand assumption 2, Exclusion: If we remove {Price}→{S}, then ¬({Price}→LC)\n",
            "\n",
            "## Realized estimand\n",
            "Realized estimand: Wald Estimator\n",
            "Realized estimand type: EstimandType.NONPARAMETRIC_ATE\n",
            "Estimand expression:\n",
            " ⎡  d       ⎤\n",
            "E⎢──────(LC)⎥\n",
            " ⎣dPrice    ⎦\n",
            "─────────────\n",
            " ⎡  d      ⎤ \n",
            "E⎢──────(S)⎥ \n",
            " ⎣dPrice   ⎦ \n",
            "Estimand assumption 1, As-if-random: If U→→LC then ¬(U →→{Price})\n",
            "Estimand assumption 2, Exclusion: If we remove {Price}→{S}, then ¬({Price}→LC)\n",
            "Estimand assumption 3, treatment_effect_homogeneity: Each unit's treatment ['S'] is affected in the same way by common causes of ['S'] and ['LC']\n",
            "Estimand assumption 4, outcome_effect_homogeneity: Each unit's outcome ['LC'] is affected in the same way by common causes of ['S'] and ['LC']\n",
            "\n",
            "Target units: ate\n",
            "\n",
            "## Estimate\n",
            "Mean value: -5.892898075037132\n",
            "\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import dowhy\n",
        "from dowhy import CausalModel\n",
        "from dowhy.datasets import linear_dataset\n",
        "\n",
        "# Let's create a sample dataset for illustration purposes\n",
        "# Normally, you'd import your own dataset: data = pd.read_csv('your_dataset.csv')\n",
        "n_points = 1000\n",
        "data = pd.DataFrame({\n",
        "    \"S\": np.random.binomial(n=1, p=0.5, size=n_points),  # 0 for non-smoker, 1 for smoker\n",
        "    \"LC\": np.random.binomial(n=1, p=0.5, size=n_points),  # 0 for no lung cancer, 1 for lung cancer\n",
        "    \"Price\": np.random.normal(loc=5, scale=1, size=n_points),  # Example cigarette price\n",
        "})\n",
        "\n",
        "# Create a causal DAG\n",
        "model=CausalModel(\n",
        "        data = data,\n",
        "        treatment='S',\n",
        "        outcome='LC',\n",
        "        common_causes=['G', 'A', 'E', 'O'],  # Potential confounders\n",
        "        instruments=['Price']  # Instrumental Variable\n",
        ")\n",
        "\n",
        "# Identify the causal effect\n",
        "identified_estimand = model.identify_effect(proceed_when_unidentifiable=True)\n",
        "\n",
        "# Estimate the causal effect using Instrumental Variable method\n",
        "estimate = model.estimate_effect(identified_estimand,\n",
        "                                 method_name=\"iv.instrumental_variable\",\n",
        "                                 method_params={'iv_instrument_name': 'Price'})\n",
        "\n",
        "# Print the causal effect estimate\n",
        "print(estimate)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Listing 13.2: Viewing example tokens than an LLM operates upon\n",
        "\n",
        "In the context of LLMs, a \"token\" refers to a sequence of characters that the model reads. A token can be as short as one character or as long as one word. Tokens are the units into which input text is divided into manageable pieces for the model.\n",
        "\n",
        "Huggingface's Transformers library has a publicly available version of GPT-2, which is far inferior to cutting-edge models, but has a similar Transformer architecture. Let's tokenize the expression “Can LLMs reason counterfactually?”"
      ],
      "metadata": {
        "id": "f9-KjHaGKcuK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import GPT2Tokenizer\n",
        "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')    #A\n",
        "tokens = tokenizer.tokenize(\"Can LLMs reason counterfactually?\")    #B\n",
        "print(tokens)    #C\n",
        "#A Initialize the GPT-2 tokenizer\n",
        "#B Tokenize the sequence\n",
        "#C Print out the tokens"
      ],
      "metadata": {
        "id": "QW2fSnevJSfB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0eada49c-21db-4181-e0c2-40c0db63a6c9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:89: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Can', 'ĠLL', 'Ms', 'Ġreason', 'Ġcounter', 'fact', 'ually', '?']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The “Ġ” corresponds to a space. Note that punctuations are tokens, and that words like “counterfactual” are broken up into multiple tokens."
      ],
      "metadata": {
        "id": "3JlNSoRhNIjy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Listing 13.3 Coverting tokens to integers\n",
        "Each token corresponds to an integer indexing the token in a large “vocabulary”. GPT-2 has a vocabulary size of 50,257.\n"
      ],
      "metadata": {
        "id": "n7DQiiVaKjAK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "input_ids = tokenizer.encode(    #A\n",
        "    \"Can LLMs reason counterfactually?\",    #A\n",
        "    return_tensors='pt'    #A\n",
        ")    #A\n",
        "print(input_ids)\n",
        "#A \"Encode\" the tokens into integers that index the token in a list of tokens called the \"vocabulary\""
      ],
      "metadata": {
        "id": "jUwQdYq2K1YP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c39b611c-013c-4a54-bec5-b1510d1e46fb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[ 6090, 27140, 10128,  1738,  3753, 22584,   935,    30]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Listing 13.4: Calculate the log-probability of each token in the sequence\n",
        "\n",
        "LLMs define a joint probability distribution on sequences of tokens. For the phrase “Can LLMs reason counterfactually?”, the model defines a probability distribution:\n",
        "\n",
        "$P(X_0 = “Can”, X_1=“LL”, X_2=“Ms”, X_3=“reason”, …, X_7 = “?”)$\n",
        "\n",
        "The models also consider the chances that this sequence ended at the question mark, rather than continuing. For that, the LLM's vocabulary includes a special token to mark the end of a sequence. For GPT-2 this is token is “<|endoftext|>”:\n",
        "\n",
        "$P(X_0 = “Can”, X_1=”LL”, X_2=”Ms”, X_3=”reason”, …, X_7 = “?”, X_8=“<|endoftext|>”)$\n",
        "\n",
        "Further, autoregressive LLMs, such as the GPT and Llama series of transformer models, model text in the order of the text sequence, i.e., they factorize this joint probability as:\n",
        "\n",
        "$P(X_0 = “Can”)*P(X_1=“LL”| X_0 = “Can”)*P(X_2=”Ms”|X_0 = “Can”, X_1=“LL”)\n",
        "…*P(X_8=“<|endoftext|>”| X_0 = “Can”, X_1=”LL”, X_2=”Ms”, X_3=”reason”, …)$\n",
        "\n",
        "We can calculate each of these probabilities on the log scale with the transformers library.\n"
      ],
      "metadata": {
        "id": "2so7_RmHK-RT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import GPT2LMHeadModel\n",
        "\n",
        "model = GPT2LMHeadModel.from_pretrained('gpt2-medium')    #A\n",
        "model.eval()    #A\n",
        "\n",
        "input_text = \"Can LLMs reason counterfactually?<|endoftext|>\"    #B\n",
        "input_ids = tokenizer.encode(input_text, return_tensors='pt')    #B\n",
        "\n",
        "with torch.no_grad():    #C\n",
        "    outputs = model(input_ids)    #C\n",
        "    logits = outputs.logits    #C\n",
        "\n",
        "log_probs = torch.nn.functional.log_softmax(logits, dim=-1)    #D\n",
        "for idx, token in enumerate(input_ids[0]):    #D\n",
        "    token_log_prob = log_probs[0][idx][token].item()    #D\n",
        "    print(f\"Token: {tokenizer.decode(token)}\" +\n",
        "           \" | Log Probability: {token_log_prob}\")    #D\n",
        "#A Initialize the GPT-2 model and set to evaluation mode.\n",
        "#B Tokenize and encode the phrase, including the end-of-sequence token.\n",
        "#C Given the phrase the model produces logits for every element in the vocabulary\n",
        "#D For each position in the sequence, get the log probability corresponding to the token that was actually present in that position\n"
      ],
      "metadata": {
        "id": "hNS1GYarLANK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0866f629-e110-4baa-ce7f-0d1e1e359c83"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Token: Can | Log Probability: {token_log_prob}\n",
            "Token:  LL | Log Probability: {token_log_prob}\n",
            "Token: Ms | Log Probability: {token_log_prob}\n",
            "Token:  reason | Log Probability: {token_log_prob}\n",
            "Token:  counter | Log Probability: {token_log_prob}\n",
            "Token: fact | Log Probability: {token_log_prob}\n",
            "Token: ually | Log Probability: {token_log_prob}\n",
            "Token: ? | Log Probability: {token_log_prob}\n",
            "Token: <|endoftext|> | Log Probability: {token_log_prob}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Summing these together provides the joint probability of the sequence under the model."
      ],
      "metadata": {
        "id": "tgrrXbojOcGM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Listing 13.5: Generation from the LLM\n",
        "\n",
        "As a generative model, GPT-2 can generate the next token conditional on the tokens that came before it. The prompt the user provides is the beginning of the sequence, and generation continues the sequence.\n"
      ],
      "metadata": {
        "id": "hfcIfZ7gLJ4Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = \"Counterfactual reasoning would enable AI to\"    #A\n",
        "input_ids = tokenizer.encode(prompt, return_tensors='pt')    #A\n",
        "\n",
        "output = model.generate(    #B\n",
        "    input_ids,    #B\n",
        "    max_length=25,    #B\n",
        "    do_sample=True,    #B\n",
        "    pad_token_id=tokenizer.eos_token_id    #B\n",
        ")    #B\n",
        "\n",
        "generated_text = tokenizer.decode(    #C\n",
        "    output[0], skip_special_tokens=True)    #C\n",
        "print(generated_text)    #C\n",
        "\n",
        "#A Specify and encode the prompt.\n",
        "#B Generate from the model. The \"do_sample=True argument means we're doing randomly selection from the probability distribution of the next token given all the previous tokens.\n",
        "#C Decode and print the output."
      ],
      "metadata": {
        "id": "h472ylfqLbVe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "81f5f6ef-a396-4845-f6ff-9623bed4ee04"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Counterfactual reasoning would enable AI to avoid a certain outcome that we cannot currently predict. That reasoning will be used to help\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Listing 13.6: Load causal narrative data\n",
        "\n",
        "In the following, we demonstrate the proof-of-concept for the idea of training using LLMs to train the causal Markov kernels in a causal model.\n",
        "\n",
        "We'll work with a training dataset of short vignettes, rather than full scripts. Let's load and examine the training data.\n"
      ],
      "metadata": {
        "id": "iyLSsxD2LTF9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "url = (\"https://raw.githubusercontent.com/altdeep/\"\n",
        "       \"causalML/master/book/chapter%2013/\"\n",
        "       \"king-prince-kingdom-updated.csv\")\n",
        "df = pd.read_csv(url)\n",
        "print(df.shape[0])    #A\n",
        "\n",
        "print(df[\"King\"][0] + \"\\n\")    #B\n",
        "print(df[\"King\"][1] + \"\\n\")    #B\n",
        "print(df[\"King\"][2])    #B\n",
        "\n",
        "print(\"----\")\n",
        "print(df[\"Prince\"][0] + \"\\n\")    #C\n",
        "print(df[\"Prince\"][1] + \"\\n\")    #C\n",
        "print(df[\"Prince\"][2])    #C\n",
        "\n",
        "print(\"----\")\n",
        "print(df[\"Kingdom\"][0] + \"\\n\")    #D\n",
        "print(df[\"Kingdom\"][1] + \"\\n\")    #D\n",
        "print(df[\"Kingdom\"][2])    #D\n",
        "#A The data has 21000 stories, broken up into three short vignettes.\n",
        "#B First, the king acts.\n",
        "#C Then the prince acts.\n",
        "#D Finally, the kingdom experiences the consequences of the royals' actions.\n"
      ],
      "metadata": {
        "id": "uYq9PeUHLoUq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9c47ae0f-b814-426a-c8dc-9eb381ec16af"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "21000\n",
            "King brokers a peace treaty with a rival kingdom, putting an end to years of bloody conflict\n",
            "\n",
            "A wise king successfully negotiates peace with a rival nation\n",
            "\n",
            "A wise king successfully negotiates peace between his kingdom and a long-time enemy\n",
            "----\n",
            "however, his son, the Prince, falls in love and marries a foreigner, causing political unrest\n",
            "\n",
            "Prince falls in love with and marries a foreign princess, forging a strong alliance\n",
            "\n",
            "but when a new threat emerges, the Prince leads the army to defend their realm\n",
            "----\n",
            "despite efforts, the ongoing war results in both kingdoms falling into poverty.\"\n",
            "\n",
            "the alliance strengthens their forces, leading the kingdom to a victorious battle.\"\n",
            "\n",
            "however, a series of misfortunes and disastrous decisions plunge their once prosperous kingdom into poverty.\"\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Listing 13.7: Training the causal LLM\n",
        "\n",
        "To train the causal Markov kernels for each node in our DAG, we'll take pre-trained models from the Huggingface Transformers library, and then further train (AKA “fine-tune”) the models using our vignettes. The pre-training took care of the heavy-lifting in terms of learning to generate coherent natural language text. The fine-tuning will align the models towards representing our causal Markov kernels.\n",
        "\n",
        "First, I'll use a GPT-2 variant to model the King's action vignettes. As a text-completion model, it typically takes a prompt as input. But I'm going to train it to generate with an empty prompt and produce vignettes according to the marginal probabilities of the King's action texts in the training data, as in the following figure.\n",
        "\n",
        "![king trained with GPT-2](https://github.com/altdeep/causalML/blob/master/book/chapter%2013/images/king-gpt-2.png?raw=1)\n",
        "\n",
        "This figure GPT-2 is fine-tuned to represent the distribution of King's action vignettes.\n",
        "Next, I'm going to use a BART sequence-to-sequence model for the causal Markov kernel Prince's action. Given the King's action vignette as input, it will generate a Prince's action vignette, as illustrated in the next issue.\n",
        "\n",
        "![king2prince model](https://github.com/altdeep/causalML/blob/master/book/chapter%2013/images/king2prince.png?raw=1)\n",
        "\n",
        "This figure says a BART sequence-to-sequence model is fine-tuned to represent Prince's action given King's action.\n",
        "We'll also use a BART sequence-to-sequence model to model the causal Markov kernel for Kingdom's fate, as shown in the next figure.\n",
        "\n",
        "![kingdom model](https://github.com/altdeep/causalML/blob/master/book/chapter%2013/images/kingdom-model.png?raw=1)\n",
        "\n",
        "A BART sequence-to-sequence model is also used to model Kingdom's fate given King's action and Prince's action.\n",
        "The model will map the King's action and Prince's action to the Kingdom's fate.\n",
        "Jumping ahead, I'm going to be interested in the conditional probability distribution of Kingdom's fate given a certain action by the Prince. Since that will require inference of the King's actions given the Prince, I'm going to additionally train one more BART model, one that generates a King's action vignette given a Prince's action vignette, as shown in the next figure.\n",
        "\n",
        "![prince2king model](https://github.com/altdeep/causalML/blob/master/book/chapter%2013/images/prince2king.png?raw=1)\n",
        "\n",
        "A BART sequence-to-sequence model is also fine-tuned to model Kingdom's fate given King's action and Prince's action.\n",
        "Let's run the training procedure. First, we'll setup our imports and our tokenizer."
      ],
      "metadata": {
        "id": "vebDMQAdLvzw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from sklearn.model_selection import train_test_split\n",
        "from torch.utils.data import Dataset\n",
        "from transformers import (\n",
        "    AutoModelForCausalLM, AutoModelForSeq2SeqLM,\n",
        "    AutoTokenizer, DataCollatorForLanguageModeling,\n",
        "    Seq2SeqTrainer, Seq2SeqTrainingArguments,\n",
        "    Trainer, TrainingArguments)\n",
        "url = (\"https://raw.githubusercontent.com/altdeep/\"\n",
        "       \"causalML/master/book/chapter%2013/\"\n",
        "       \"king-prince-kingdom-updated.csv\")\n",
        "df = pd.read_csv(url)\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"facebook/bart-base\")    #A\n",
        "tokenizer.pad_token = tokenizer.eos_token    #A\n",
        "def tokenize_phrases(phrases, max_length=40):    #A\n",
        "    return tokenizer(    #A\n",
        "        phrases,    #A\n",
        "        truncation=True,    #A\n",
        "        padding='max_length',    #A\n",
        "        max_length=max_length    #A\n",
        "    )    #A\n",
        "#A Set up the tokenizer. We'll use Bart-base as the tokenizer for all of our models. The pad token is used to make all the sequences the same length to facilitate matrix operations. It is common to set it to the \"end-of-sequence (EOS)\" token. The max length of the token is set to 40, as all of the vignettes are less that 40 tokens."
      ],
      "metadata": {
        "id": "WpEkPpNhLvB6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "085ade51-979d-482d-ed94-6a96417ec06e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Listing 13.8: Tokenizing the King vignettes\n",
        "\n",
        "Next, we create a class and a function that tokenizes the King dataset. I use `ModelDataset`, a custom subclass of the PyTorch Dataset class, designed to store token encodings and their corresponding labels. When accessed by index, it returns a dictionary containing token encodings for that index and the associated label, and it provides the total number of examples via its length method.\n"
      ],
      "metadata": {
        "id": "hprEwsTxL6Gw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ModelDataset(Dataset):    #A\n",
        "    def __init__(self, encodings, labels):    #A\n",
        "        self.encodings = encodings    #A\n",
        "        self.labels = labels    #A\n",
        "    #A\n",
        "    def __getitem__(self, idx):    #A\n",
        "        item = {    #A\n",
        "            key: torch.tensor(val[idx])    #A\n",
        "            for key, val in self.encodings.items()    #A\n",
        "        }    #A\n",
        "        item['labels'] = torch.tensor(self.labels[idx])    #A\n",
        "        return item    #A\n",
        "    #A\n",
        "    def __len__(self):    #A\n",
        "        return len(self.encodings.input_ids)    #A\n",
        "    #A\n",
        "def create_king_dataset(input_phrases):    #B\n",
        "    king_phrases = input_phrases.tolist()    #B\n",
        "    king_encodings = tokenize_phrases(king_phrases)    #B\n",
        "    king_dataset = ModelDataset(\n",
        "        king_encodings, king_encodings['input_ids'])    #B\n",
        "    return king_dataset    #B\n",
        "#A When accessed by index, ModelDataset returns a dictionary containing token encodings for that index and the associated label,\n",
        "#B Create a ModelDataset instance for the king vignettes."
      ],
      "metadata": {
        "id": "x7DrE9EvMEL0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Listing 13.9: Tokenizing Prince and Kingdom vignettes\n",
        "\n",
        "Next we'll tokenize the Prince and Kingdom vignettes. This code will also produce a validation dataset used in training sequence-to-sequence models."
      ],
      "metadata": {
        "id": "yJxNyr6rMK0a"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def create_seq2seq_datasets(input_phrases, target_phrases):\n",
        "    input_phrases_list = input_phrases.tolist()\n",
        "    target_phrases_list = target_phrases.tolist()\n",
        "    spit = train_test_split(    #A\n",
        "        input_phrases_list,    #A\n",
        "        target_phrases_list,    #A\n",
        "        test_size=0.1    #A\n",
        "    )    #A\n",
        "    train_inputs, val_inputs, train_targets, val_targets = spit    #A\n",
        "    train_input_encodings = tokenize_phrases(train_inputs)    #B\n",
        "    val_input_encodings = tokenize_phrases(val_inputs)    #B\n",
        "    train_target_encodings = tokenize_phrases(train_targets)    #B\n",
        "    val_target_encodings = tokenize_phrases(val_targets)    #B\n",
        "    train_dataset = ModelDataset(\n",
        "        train_input_encodings, train_target_encodings['input_ids']\n",
        "    )\n",
        "    val_dataset = ModelDataset(\n",
        "        val_input_encodings, val_target_encodings['input_ids']\n",
        "    )\n",
        "    return train_dataset, val_dataset\n",
        "#A Splits input and target phrases into training and validation sets.\n",
        "#B Encode the training and validation sets.\n"
      ],
      "metadata": {
        "id": "5D0Lp-VzMQtw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Listing 13.10: Training the King model\n",
        "\n",
        "Next, we’ll write a training algorithm for the King model. This function initializes a GPT2 model with the specified parameters, sets up the training arguments, and trains the model on the provided dataset, finally saving the trained model to the specified directory."
      ],
      "metadata": {
        "id": "7H0P-cV5MVl3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train_king_model(output_dir, train_dataset,\n",
        "                     model_name=\"gpt2-medium\", epochs=4):\n",
        "    king_model = AutoModelForCausalLM.from_pretrained(model_name)    #A\n",
        "    training_args_king = TrainingArguments(    #A\n",
        "      output_dir=output_dir,    #A\n",
        "      per_device_train_batch_size=32,    #A\n",
        "      overwrite_output_dir=True,    #A\n",
        "      num_train_epochs=epochs,    #A\n",
        "      save_total_limit=1,    #A\n",
        "      save_steps=len(train_dataset) // 16,\n",
        "      max_grad_norm=1.0\n",
        "    )    #A\n",
        "    data_collator = DataCollatorForLanguageModeling(    #A\n",
        "        tokenizer=tokenizer, mlm=False)    #A\n",
        "    trainer_king = Trainer(    #B\n",
        "        model=king_model,    #B\n",
        "        args=training_args_king,    #B\n",
        "        data_collator=data_collator,    #B\n",
        "        train_dataset=train_dataset,    #B\n",
        "    )    #B\n",
        "    trainer_king.train()    #C\n",
        "    king_model.save_pretrained(output_dir)\n",
        "    return king_model\n",
        "#A Initialize and configure model with the specified parameters.\n",
        "#B Configure the training settings.\n",
        "#C Trains the model."
      ],
      "metadata": {
        "id": "eyy490usMbtx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Listing 13.11: Training the sequence-to-sequence models\n",
        "\n",
        "Next, we write a training algorithm for the sequence-to-sequence models. The function splits the provided input and target phrases into training and validation sets, tokenizes them, and then creates and returns PyTorch Dataset objects for both sets using the ModelDataset class. The `train_seq2seq_model` function initializes a sequence-to-sequence model with the specified parameters, configures its training settings, and then trains the model using both training and validation datasets, finally returning the trained model.\n"
      ],
      "metadata": {
        "id": "nrIZJ0aWMgiL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train_seq2seq_model(output_dir, train_dataset, val_dataset,\n",
        "                        model_name=\"facebook/bart-base\",\n",
        "                        epochs=4):\n",
        "    model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
        "    training_args = Seq2SeqTrainingArguments(    #A\n",
        "        output_dir=output_dir,    #A\n",
        "        per_device_train_batch_size=16,    #A\n",
        "        predict_with_generate=True,    #A\n",
        "        logging_dir=f\"{output_dir}/logs\",    #A\n",
        "        save_total_limit=1,    #A\n",
        "        save_steps=len(train_dataset) // 16,    #A\n",
        "        learning_rate=3e-5,    #A\n",
        "        num_train_epochs=epochs,    #A\n",
        "        warmup_steps=500,    #A\n",
        "        weight_decay=0.01,    #A\n",
        "    )    #A\n",
        "    trainer = Seq2SeqTrainer(    #B\n",
        "        model=model,    #B\n",
        "        args=training_args,    #B\n",
        "        train_dataset=train_dataset,    #B\n",
        "        eval_dataset=val_dataset,    #B\n",
        "    )    #B\n",
        "    trainer.train()    #C\n",
        "    model.save_pretrained(output_dir)\n",
        "    return model\n",
        "#A Initialize and configure sequence-to-sequence model with the specified parameters.\n",
        "#B Configure the training settings.\n",
        "#C Trains the model using both training and validation datasets, finally returning the trained model."
      ],
      "metadata": {
        "id": "nBT2Dk5gMpu5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Listing 13.12: Training the King, Prince, and Kingdom models\n",
        "\n",
        "Now we train the models. We'll specify some directories for saving checkpoints."
      ],
      "metadata": {
        "id": "XncFTnd_MyZS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "king_model_path = os.path.join(os.getcwd(), 'king_model')    #A\n",
        "prince_model_path = os.path.join(os.getcwd(), 'prince_model')    #A\n",
        "kingdom_model_path = os.path.join(os.getcwd(), 'kingdom_model')    #A\n",
        "prince2king_model_path = os.path.join(    #A\n",
        "    os.getcwd(), 'prince2king_model')    #A\n",
        "\n",
        "king_dataset = create_king_dataset(df[\"King\"])    #B\n",
        "king_model = train_king_model(king_model_path, king_dataset)    #B\n",
        "\n",
        "datasets = create_seq2seq_datasets(df[\"King\"], df[\"Prince\"])    #C\n",
        "train_dataset_prince, val_dataset_prince = datasets\n",
        "prince_model = train_seq2seq_model(    #C\n",
        "    prince_model_path,    #C\n",
        "    train_dataset_prince,    #C\n",
        "    val_dataset_prince,    #C\n",
        "    epochs=6    #C\n",
        ")    #C\n",
        "\n",
        "king_and_prince = [f\"{k} {p}\" for k, p in zip(df[\"King\"], df[\"Prince\"])]    #D\n",
        "df[\"King and Prince\"] = king_and_prince    #D\n",
        "train_dataset_kingdom, val_dataset_kingdom = create_seq2seq_datasets(    #D\n",
        "    df[\"King and Prince\"], df[\"Kingdom\"]    #D\n",
        ")    #D\n",
        "kingdom_model = train_seq2seq_model(    #D\n",
        "    kingdom_model_path,    #D\n",
        "    train_dataset_kingdom,    #D\n",
        "    val_dataset_kingdom,    #D\n",
        "   epochs=6    #D\n",
        ")    #D\n",
        "#A Provide the output directories where you want to save your model.\n",
        "#B Train the King model using Seq2Seq.\n",
        "#C Train the Prince model using Seq2Seq. The King vignettes are used to predict the Prince vignettes.\n",
        "#D Train the Kingdom model using Seq2Seq. The combined King and Prince vignettes are used to predict the Kingdom vignettes."
      ],
      "metadata": {
        "id": "1WFZ4jkLM2wp",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 382
        },
        "outputId": "9255f815-01cc-4f2c-85ad-cfe6eeff7adf"
      },
      "execution_count": null,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/accelerate/accelerator.py:432: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: \n",
            "dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='1352' max='2628' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [1352/2628 06:50 < 06:28, 3.29 it/s, Epoch 2.06/4]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>500</td>\n",
              "      <td>1.796300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1000</td>\n",
              "      <td>1.211100</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Checkpoint destination directory /content/king_model/checkpoint-1312 already exists and is non-empty. Saving will proceed but saved results may be invalid.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='1541' max='2628' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [1541/2628 07:45 < 05:29, 3.30 it/s, Epoch 2.34/4]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>500</td>\n",
              "      <td>1.796300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1000</td>\n",
              "      <td>1.211100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1500</td>\n",
              "      <td>1.089400</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Listing 13.13: Function to train the Prince-to-King model\n",
        "\n",
        "Finally, we train the model for inferring the King vignette given a Prince Vignette. We'll use this in inference later."
      ],
      "metadata": {
        "id": "aGVvACG4M68F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "p2k_data = create_seq2seq_datasets(    #A\n",
        "    df[\"Prince\"], df[\"King\"])    #A\n",
        "train_dataset_prince2king, val_dataset_prince2king = p2k_data    #A\n",
        "prince2king_model = train_seq2seq_model(    #A\n",
        "    prince2king_model_path,    #A\n",
        "    train_dataset_prince2king,    #A\n",
        "    val_dataset_prince2king,    #A\n",
        "    epochs=6    #A\n",
        ")    #A\n",
        "#A Train another Seq2Seq model that predicts King given Prince."
      ],
      "metadata": {
        "id": "9o9ryk6wM_u4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 906,
          "referenced_widgets": [
            "b20832df4ba74230948b5bcdf2426ba4",
            "7490650b8df94e86b801af13906768bc",
            "5bd8820b76454149811868122974bfae",
            "e94021439d2f47269ef96476f71cea93",
            "fd482688e58c4880aaaf71ae24a83fb8",
            "02cee6e1e25645c3ac24f6b1400cc6f5",
            "671957c0838d4df198968bbd9245f003",
            "f1d8a545df504e379f1c4f6033de30fa",
            "03a5491c06804b3fa0e5be9aefdb54be",
            "81a7ce8eb4b145baa2a8328aff10ec31",
            "5d783e33ec6d45a38349ea9fd1279603"
          ]
        },
        "outputId": "95b8fed5-50c7-42e1-b54d-e79ebc1f89a8"
      },
      "execution_count": null,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "b20832df4ba74230948b5bcdf2426ba4",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/558M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/accelerate/accelerator.py:432: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: \n",
            "dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='7092' max='7092' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [7092/7092 09:29, Epoch 6/6]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>500</td>\n",
              "      <td>0.675200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1000</td>\n",
              "      <td>0.389100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1500</td>\n",
              "      <td>0.363600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2000</td>\n",
              "      <td>0.353800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2500</td>\n",
              "      <td>0.341600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3000</td>\n",
              "      <td>0.332500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3500</td>\n",
              "      <td>0.329400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4000</td>\n",
              "      <td>0.322800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4500</td>\n",
              "      <td>0.320400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5000</td>\n",
              "      <td>0.315500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5500</td>\n",
              "      <td>0.311900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6000</td>\n",
              "      <td>0.310300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6500</td>\n",
              "      <td>0.308100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>7000</td>\n",
              "      <td>0.307200</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
            "Non-default generation parameters: {'early_stopping': True, 'num_beams': 4, 'no_repeat_ngram_size': 3, 'forced_bos_token_id': 0, 'forced_eos_token_id': 2}\n",
            "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
            "Non-default generation parameters: {'early_stopping': True, 'num_beams': 4, 'no_repeat_ngram_size': 3, 'forced_bos_token_id': 0, 'forced_eos_token_id': 2}\n",
            "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
            "Non-default generation parameters: {'early_stopping': True, 'num_beams': 4, 'no_repeat_ngram_size': 3, 'forced_bos_token_id': 0, 'forced_eos_token_id': 2}\n",
            "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
            "Non-default generation parameters: {'early_stopping': True, 'num_beams': 4, 'no_repeat_ngram_size': 3, 'forced_bos_token_id': 0, 'forced_eos_token_id': 2}\n",
            "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
            "Non-default generation parameters: {'early_stopping': True, 'num_beams': 4, 'no_repeat_ngram_size': 3, 'forced_bos_token_id': 0, 'forced_eos_token_id': 2}\n",
            "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
            "Non-default generation parameters: {'early_stopping': True, 'num_beams': 4, 'no_repeat_ngram_size': 3, 'forced_bos_token_id': 0, 'forced_eos_token_id': 2}\n",
            "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
            "Non-default generation parameters: {'early_stopping': True, 'num_beams': 4, 'no_repeat_ngram_size': 3, 'forced_bos_token_id': 0, 'forced_eos_token_id': 2}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Listing 13.14: Pull transformer models from Hugginface Hub and generate\n",
        "\n",
        "Running the above training code takes some time, especially if not using GPU. You can download versions of the trained models from Huggingface Hub. The following code pulls the transformer models from Huggingface Hub."
      ],
      "metadata": {
        "id": "QEvzljQjNExW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import torch\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from transformers import (\n",
        "    AutoModelForCausalLM, AutoModelForSeq2SeqLM,\n",
        "    AutoTokenizer, GPT2LMHeadModel,\n",
        "    PreTrainedModel, BartForConditionalGeneration)\n",
        "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "king_model = AutoModelForCausalLM.from_pretrained(    #A\n",
        "    \"osazuwa/causalLLM-king\").to(DEVICE)    #A\n",
        "prince_model = AutoModelForSeq2SeqLM.from_pretrained(    #A\n",
        "    \"osazuwa/causalLLM-prince\").to(DEVICE)    #A\n",
        "kingdom_model = AutoModelForSeq2SeqLM.from_pretrained(    #A\n",
        "    \"osazuwa/causalLLM-kingdom\").to(DEVICE)    #A\n",
        "prince2king_model = AutoModelForSeq2SeqLM.from_pretrained(    #A\n",
        "    \"osazuwa/causalLLM-prince2king\").to(DEVICE)    #A\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"facebook/bart-base\")    #B\n",
        "tokenizer.pad_token = tokenizer.eos_token    #B\n",
        "#A Load the components of our model.\n",
        "#B Load Bart-base tokenizer and set pad token to end-of-sequence tokens."
      ],
      "metadata": {
        "id": "LJbg-lNkM_63",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 457,
          "referenced_widgets": [
            "c0ea054083294fa78410bfaa7333629f",
            "bc12ade2d9124673be92767eaccf259c",
            "868d5daf4546421d9650872367455788",
            "c138b9896bfd4d27a2058e6eb6a6a8cc",
            "ef7035503cac438b85fb6b6b46ecab53",
            "7edb1d0232c64e39bd28e81d32f894e8",
            "95953358885d440a89d2b54ba4d292b1",
            "d51c1ed69b6a4fde9287ceefa57ec959",
            "224e8376558c42e492db67ae639354c5",
            "893c9e016faa46c783f608065b7d90e9",
            "6d7b128419ce4239936a41c75b04335a",
            "42f580a5a6994bf79c3465969f29042f",
            "a53abc75268e4660afa37f5017868643",
            "c98a4cd3f1d042e5941934b8f36a6b10",
            "5b1949556b1648a98490bc038fa20c2c",
            "3dd832d382254f4eba8780c605d084ea",
            "0b610fb3f6a7458dbf2b8abf8f536bda",
            "7f844f85070c49c18175f3b910602ab8",
            "12375d3258e94baea86e921f8aef953c",
            "07c30d313ef0476ba84f124082d0adfb",
            "caa385259c7c493f9c32a06bccda43ef",
            "51da5272cab44be8a297ee4b9117b22d",
            "ec1db11cda6a4fb9aa4f1e18a7a73eb2",
            "4a94b7ac72404c37a665e8bfaeb1d75e",
            "08a87157a31e49728e84ef6963889df2",
            "4e1f977594b74bde80e33d34f95b598f",
            "a458154ca3d847b1b2861ee810fb3ecb",
            "c7052e750ada435cafe072926daf8da0",
            "b53bee19d51349b1b8785464ea076de0",
            "0ad15821779d4a0ca7a8411ebd4c3bd8",
            "ddcfb7c135d645398a78cf32c90bd677",
            "aa7579b651954efa9a3352e73bff1417",
            "e5d667ef172442e7bd8441d1d16871b8",
            "bce86adf219e494d9a3b3fa6a54e6828",
            "d1fd91d7a46a40ff988caca7b8fca297",
            "cfae591244a249839ec4a32c0698c8de",
            "feff1c9dab154482b7df2cff512a7a05",
            "0884972cb73c416b9c748cf1236996af",
            "3d7bc88daf1b4b82a4e3107e6a1e151f",
            "f99a46e139f9459b8cea1a81e9fa4674",
            "c0c27f85e3a148b68f9a845b1cbbce57",
            "afb6c78d5d034e6383909a2b7b81b5bb",
            "3799ea95132b4381b3d1e604a3d4e03f",
            "52171f6d3214425fbf6eeefed4cc1c59",
            "50a8d8defba24e27b23193c64a62d28c",
            "25eb2ca2a6a14b6b88220c71db5140c8",
            "3be4e177351047d0956e0518e817a305",
            "257e8e8497bc4705882684119dde994d",
            "f9533663b50742f9a5a15daccf73f5d7",
            "f7c0e87943634e3993c75728f3ae02b8",
            "ad668d8a8f2a416fa9651ee67d6a6080",
            "f30a642b59ce4bc89f53b89cc910c390",
            "7e3eae99f34347cea607c82cc9e4fc44",
            "bac91c4960054a1bbb51746f7244c2c6",
            "db5069def10246c9be159f1b80fb67ae",
            "e69b84ffc48f4dcbbc547f70eacdf756",
            "c62fcce3e0d7477ab70cae8be955b813",
            "4ec02c08d8204b10b84bcd3ea6ff8dbb",
            "405c01e471e0498c809d8de5b0e17cd7",
            "01fc9ce77e9a4d5daf8f6a8435c82837",
            "eb79f81a86a14e0ca07fcd021f2f939f",
            "8f20994296d74ef58ea70afd8d163dbd",
            "2a1f29d6bf7744ba8299493a0df51953",
            "0889cde6ea6c479d999d3d27f82112c8",
            "5cf6410088a5454282f54dca0a38cc95",
            "4eb01a97c12e4dd38483ba99a2f5f140",
            "0bf4f6ae3dfb48689ee6ee764cbc200c",
            "069b8df1f7a24baa945a1aca91317bb9",
            "fcc22e730c0e42f0824785c6ad7aa9b2",
            "ef863f282b9946b1a15131b86102b999",
            "21480c72a77b40b8a89eff7963d0c0c6",
            "34c3aecf777640c5899d8e4f40a6be6e",
            "356bc7383f8f45628d89d1266ee61417",
            "1e60a55d08bc4824b420ec3d9b51acb1",
            "60594abb7fe8492f9be5b8b856f66cf5",
            "39536c7e9dc84f5b86b9e56f466a6725",
            "d7ed5f6ac55a484d87aa3201459de8d9",
            "1ddbb5268f6e42eeb1b1afa37c68ca35",
            "1a188caec9474410923ae1a7e26d5d4c",
            "032dd97392634b838d116295bf4a89ac",
            "c717737b86fd476a96c0276100bd33c7",
            "0a69debef1c541b8b00ea99dd97dfb17",
            "d2390330e2394341bbbba905af4ea602",
            "99d5f022c0294f579daed4f614d05b8e",
            "e5e47b11645f4a8d94e2687ca4a7f8ad",
            "3c886b5705aa4c1299138f3d9110315c",
            "859f0b62fed6481b8add76ddc0b09d53",
            "6b69704a26d847778d77d4dbb6245360",
            "8d312173082341a5af71581411b227e0",
            "7bcf8a50c96a42e2ab1b99e949d37bbe",
            "04eed9e018de4bdc8f1582f5c3ec95b1",
            "b8367fa25ba94348befc0e3716f82294",
            "8762b1ab0b8b4c2eaa23d372b7b3cb82",
            "47d11c39a5534ca6869e0a6052aa6aef",
            "358339502820402da17ae44c36dca799",
            "05f51fa9a1664733b33f9104e2b6bd16",
            "e218378b23b743f58d29b5fca91e453f",
            "5280dff288954b899cd179b353e5f560",
            "b225425a9e5f451b924fc63381ee7421",
            "b67e254e1f374c4c984782a77965a137",
            "e0793fa9495a4bcd98fef7bf90671c4f",
            "1dad45e0c4b944eba374429c1803bde8",
            "f5a37187a76b40599e6dc1cce02799a3",
            "1e94432cb7d14fb684eefec7666103ae",
            "0f3d1bd094034a9cb16cbc07f84ff63f",
            "0177b5fa73b843bdab228b1febaa7ec4",
            "faf37a15c0d24029847e52afdc8018e8",
            "2b942fcfa7e54ca286fd86b5f68315cf",
            "4172e46c41ab4a488400b0becd2cff79",
            "8ce0bf1bb2534908876b78c06084b61e",
            "bce71a7b1d9445728d142ab1f1433015",
            "58cf354ec34b4c76b59a5f2b8df3b822",
            "9bae855d39604fa380116de7b48232e2",
            "7b0b64d59fa742618558487e49459bcf",
            "ab8a30f95fba415b85f2938adb157177",
            "ecc1dde7eaf94305bd024ab54d0e2288",
            "1f2eb5bb6dd7457e8aebdf4453f50069",
            "59aebe5fd74e4cf6a1e1a1e46f906d0c",
            "5029d2bc4c3e4e1bb90515e345889844",
            "cbfc533b955141b9a5a735382bc041e3",
            "a37e506607e347908019a0fe9859aae9",
            "a926245406cd4884ba9c84ca5cb33760",
            "63fda89fab6e412fbfc664a20b7ed368",
            "fa3b9ac52f65486e898cbaab4ddb968b",
            "3f701ff1e2474dd9b495cb31f2fc3c19",
            "556af8076fd74961a18a977f669f4e5e",
            "972ef23aecc64f709f67097bf27bb8a0",
            "bb68ce1e02aa4316a95df63185aec012",
            "36c10b5d12a24131916289e2f8c599e0",
            "238d93ab80f847d8ae59c74e5becf5fc",
            "53a80347f3384896b355dd743a619ab5",
            "facae74a5fcd4d2da415985e5ff77471"
          ]
        },
        "outputId": "e258906c-1354-4360-a11c-d129133ca6c6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config.json:   0%|          | 0.00/967 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "c0ea054083294fa78410bfaa7333629f"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "pytorch_model.bin:   0%|          | 0.00/1.42G [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "42f580a5a6994bf79c3465969f29042f"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "generation_config.json:   0%|          | 0.00/119 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "ec1db11cda6a4fb9aa4f1e18a7a73eb2"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config.json:   0%|          | 0.00/1.71k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "bce86adf219e494d9a3b3fa6a54e6828"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "pytorch_model.bin:   0%|          | 0.00/1.63G [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "50a8d8defba24e27b23193c64a62d28c"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "generation_config.json:   0%|          | 0.00/292 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "e69b84ffc48f4dcbbc547f70eacdf756"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config.json:   0%|          | 0.00/1.71k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "0bf4f6ae3dfb48689ee6ee764cbc200c"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "pytorch_model.bin:   0%|          | 0.00/1.63G [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "1ddbb5268f6e42eeb1b1afa37c68ca35"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "generation_config.json:   0%|          | 0.00/292 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "8d312173082341a5af71581411b227e0"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config.json:   0%|          | 0.00/1.71k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "b67e254e1f374c4c984782a77965a137"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "pytorch_model.bin:   0%|          | 0.00/1.63G [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "bce71a7b1d9445728d142ab1f1433015"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "generation_config.json:   0%|          | 0.00/292 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "a926245406cd4884ba9c84ca5cb33760"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Listing 13.15: Helper functions for encoding, decoding, and generation\n",
        "\n",
        "Next, we write some functions to encode text to tokens, decode tokens to text and generate from the model."
      ],
      "metadata": {
        "id": "WYS5DwLINUsL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def encode(text:str, device=DEVICE) -> torch.tensor:    #A\n",
        "    input_ids = tokenizer.encode(text, return_tensors=\"pt\")    #A\n",
        "    input_ids = input_ids.to(device)    #A\n",
        "    return input_ids    #A\n",
        "\n",
        "def decode(text_ids: torch.tensor) -> str:    #B\n",
        "    output = tokenizer.decode(text_ids, skip_special_tokens=True)    #B\n",
        "    return output    #B\n",
        "\n",
        "EMPTY_TEXT = torch.tensor(tokenizer.encode(\"\")).unsqueeze(0).to(DEVICE)    #C\n",
        "\n",
        "def generate_from_model(model: PreTrainedModel,    #D\n",
        "                        input_sequence: torch.tensor = EMPTY_TEXT,    #D\n",
        "                        max_length: int = 25,    #D\n",
        "                        temperature=1.0):    #D\n",
        "    output = model.generate(    #D\n",
        "        input_sequence,    #D\n",
        "        max_length=max_length,    #D\n",
        "        do_sample=True,    #D\n",
        "        pad_token_id=tokenizer.pad_token_id,    #D\n",
        "        eos_token_id=tokenizer.pad_token_id,    #D\n",
        "        temperature=temperature,    #D\n",
        "        top_p=0.9,    #D\n",
        "    )    #D\n",
        "    return output    #D\n",
        "\n",
        "def convert_to_text(output):\n",
        "    return decode(output[0]).strip().capitalize()\n",
        "#A Encode text into tensor\n",
        "#B Decode tensor into text.\n",
        "#C Get the encoding for a comma and for empty text for convenience.\n",
        "#D A function for generating from models. These parameters do slightly different things for the GPT-2 and the BART models, but they more or less overlap."
      ],
      "metadata": {
        "id": "RkGskmyLNZ9P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Listing 13.16: Computing log-probabilities of generated sequences\n",
        "\n",
        "We want to use a probabilistic ML approach. So we need a way of computing the log probabilities of generated sequences so we can use these in inference."
      ],
      "metadata": {
        "id": "Xpx2GXTRNkPS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_log_probs(model, output_sequence):\n",
        "    if isinstance(model, GPT2LMHeadModel):    #A\n",
        "        outputs = model(    #A\n",
        "            input_ids=output_sequence,    #A\n",
        "            labels=output_sequence    #A\n",
        "        )    #A\n",
        "        log_softmax = torch.nn.functional.log_softmax(    #A\n",
        "            outputs.logits, dim=-1)    #A\n",
        "        log_probs = log_softmax.gather(2, output_sequence.unsqueeze(-1))    #A\n",
        "        log_probs = log_probs.squeeze(-1).sum(dim=-1)    #A\n",
        "    elif isinstance(model, BartForConditionalGeneration):\n",
        "        outputs = model(    #B\n",
        "            input_ids=output_sequence,    #B\n",
        "            labels=output_sequence)    #B\n",
        "        loss = outputs.loss    #B\n",
        "        log_probs = -loss * output_sequence.size(1)    #B\n",
        "    else:\n",
        "        raise ValueError(\"Unsupported model type\")\n",
        "    return torch.tensor(log_probs.item())\n",
        "#A Convert logits to logprobs for GPT2\n",
        "#B Convert logits to logprobs from BART cross-entropy"
      ],
      "metadata": {
        "id": "zOFnSyf8Nw_J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Listing 13.17: Generating a full story\n",
        "\n",
        "Finally, we put these pieces together to generate a full story from our three model."
      ],
      "metadata": {
        "id": "0QTUQIReN3-d"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "king_output = generate_from_model(king_model)    #A\n",
        "king_statement = convert_to_text(king_output)    #A\n",
        "print(\"Generated from king_nodel:\", king_statement)    #A\n",
        "log_prob_king = compute_log_probs(king_model, king_output)    #A\n",
        "print(\"Log prob of generated king text:\", log_prob_king)    #A\n",
        "\n",
        "prince_output = generate_from_model(prince_model, king_output)    #B\n",
        "prince_statement = convert_to_text(prince_output)    #B\n",
        "print(\"Generated from prince_model:\", prince_statement)    #B\n",
        "log_prob_prince = compute_log_probs(prince_model, prince_output)    #B\n",
        "print(\"Log prob of generated prince text:\", log_prob_prince)    #B\n",
        "\n",
        "king_prince_statement = king_statement + \". \" + prince_statement    #C\n",
        "king_prince_output = encode(king_prince_statement)    #C\n",
        "kingdom_output = generate_from_model(kingdom_model, king_prince_output)    #C\n",
        "kingdom_statement = convert_to_text(kingdom_output)    #C\n",
        "\n",
        "print(\"Generated from kingdom model:\", kingdom_statement)    #C\n",
        "log_prob_kingdom = compute_log_probs(kingdom_model, kingdom_output)    #C\n",
        "print(\"Log prob of generated kingdom text:\", log_prob_kingdom)    #C\n",
        "\n",
        "king_output_infer = generate_from_model(prince2king_model, prince_output)    #D\n",
        "king_statement_infer = convert_to_text(king_output_infer)    #D\n",
        "print(\"Generated statement from prince2king:\", king_statement_infer)    #D\n",
        "log_prob_prince2king = compute_log_probs(prince2king_model, prince_output)    #D\n",
        "print(\"Log prob of generated inference text:\", log_prob_prince2king)    #D\n",
        "#A Generate from the GPT-based model of vignettes about the King and calculate the log probabilities of the generated sequence.\n",
        "#B Generate from the BART-based sequence-to-sequence model of that generates vignettes about the Prince given vignettes about the King, and then calculate the log probability of the generated sequence.\n",
        "#C Generate from the BART-based sequence-to-sequence model of that generates vignettes about the Kingdom given vignettes about the King and the Prince, and then calculate the log probability of the generated sequence.\n",
        "#D Another BART-based sequence-to-sequence model that maps a vignette about the Prince to a vignette about the King. We'll use this to infer the vignette about the King from a vignette about the Prince.\n"
      ],
      "metadata": {
        "id": "zNF8WKOeN-u_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "aebbe931-df83-4882-a4bd-0c29b5dda088"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generated from king_nodel: King, fueled by ambition, declares war on a neighboring realm, recklessly declares war on a neighboring kingdom to expand\n",
            "Log prob of generated king text: tensor(-338.7216)\n",
            "Generated from prince_model: His son, the prince, disillusioned by his father's actions, abdicates the throne in\n",
            "Log prob of generated prince text: tensor(-30.6349)\n",
            "Generated from kingdom model: As the war drags on, the kingdom's resources are depleted, plunging the once-prosper\n",
            "Log prob of generated kingdom text: tensor(-63.2856)\n",
            "Generated statement from prince2king: A king, driven by ambition, declares war on a neighboring realm\n",
            "Log prob of generated inference text: tensor(-296.7132)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Listing 13.18 Building a Torch distribution in Pyro\n",
        "\n",
        "Next, we use these transformer models to define distributions in Pyro, then use Pyro to build a causal generative model. We'll use Pyro's `TorchDistributionMixin` to model the causal Markov kernels with the language models. We'll use the GPT-2 model of the king vignettes to create the causal Markov kernel of the \"King\" variable.\n",
        "\n",
        "Next, we use the BART model to create the causal Markov kernel for the \"Prince\" variable. The King variable causes this variable, so the seq2seq model uses the king variable value to generate a value for this model."
      ],
      "metadata": {
        "id": "s5c2Ep3qOMrS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pyro\n",
        "from pyro.distributions.torch_distribution \\\n",
        "import TorchDistributionMixin\n",
        "\n",
        "class TransformerModelDistribution(TorchDistributionMixin):\n",
        "    def __init__(self, model: PreTrainedModel,\n",
        "                 input_encoding: torch.tensor = EMPTY_TEXT,\n",
        "                ):\n",
        "        super().__init__()\n",
        "        self.model = model\n",
        "        self.input_encoding = input_encoding\n",
        "\n",
        "    def sample(self, sample_shape=torch.Size()):    #A\n",
        "        output = generate_from_model(    #A\n",
        "            self.model, self.input_encoding    #A\n",
        "        )    #A\n",
        "        return output    #A\n",
        "\n",
        "    def log_prob(self, value):    #B\n",
        "        return compute_log_probs(self.model, value)    #B\n",
        "#A We'll use TorchDistributionMixin to turn a transformer model in to a Pyro distribution. TorchDistributionMixin is used to make PyTorch distributions compatible with Pyro's utilities.\n",
        "#B The log_prob method returns the log probabilities used in inference algorithms.\n"
      ],
      "metadata": {
        "id": "lpMZ1wmTOMMb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Listing 13.19 Incorporating transformer models into a causal model with Pyro\n",
        "\n",
        "Finally, we create the causal Markov kernel for the Kingdom variable. The King and Prince variables are causal parents, so we concatenate their generated outputs into one string, and use that string to generate the Kingdom output again using a BART seq2seq model. We rely on mixin called `TorchDistribution`, which is useful for wrapping PyTorch distributions for use in Pyro."
      ],
      "metadata": {
        "id": "ajjWJ5PJb-zo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def causalLLM():    #A\n",
        "    king = pyro.sample(    #B\n",
        "        \"King\", TransformerModelDistribution(king_model)    #B\n",
        "    )    #B\n",
        "    prince = pyro.sample(    #C\n",
        "        \"Prince\", TransformerModelDistribution(    #C\n",
        "            prince_model, king)    #C\n",
        "    )    #C\n",
        "    king_and_prince = torch.cat([king, prince], dim=1)    #D\n",
        "    kingdom = pyro.sample(    #D\n",
        "        \"Kingdom\", TransformerModelDistribution(    #D\n",
        "            kingdom_model, king_and_prince)    #D\n",
        "    )    #D\n",
        "    king_text = convert_to_text(king)    #E\n",
        "    prince_text = convert_to_text(prince)    #E\n",
        "    kingdom_text = convert_to_text(kingdom)    #E\n",
        "    return king_text, prince_text, kingdom_text    #E\n",
        "\n",
        "for _ in range(2):    #F\n",
        "    king, prince, kingdom = causalLLM()    #F\n",
        "    vignette = \" \".join([king, prince, kingdom])    #F\n",
        "    print(vignette)    #F\n",
        "\n",
        "#A Now we build the causal LLM.\n",
        "#B Create the causal Markov kernel of the \"King\" variable.\n",
        "#C Create the causal Markov kernel of the “Prince” variable.\n",
        "#D Create the causal Markov kernel for the Kingdom variable.\n",
        "#E We concatenate all the generated vignettes into one overall vignettes and return the result.\n",
        "#F We confirm our causal model generates the full vignette."
      ],
      "metadata": {
        "id": "FTpATFX9Ax0-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8f9cebe1-99a9-4d57-d982-56faaa5da76a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "King, driven by a thirst for power, declares war on a neighboring realm, finally declares war on a neighboring realm The prince, disillusioned by his father's actions, abdicates the throne in protest Without strong leadership, the kingdom falls into poverty and despair.\"\n",
            "King, driven by ambition, declares war on a neighboring nation to expand his kingdom's territories, declares war on a The prince, disillusioned by his father's actions, abdicates the throne in protest As the war drags on, the kingdom's resources are depleted, plunging its people into poverty\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Listing 13.20: Proposal distribution for P(Kingdom|Prince=x)\n",
        "\n",
        "The distribution $P(\\text{Kingdom}|\\text{Prince}=x)$ will be different from $P(\\text{Kingdom}_{\\text{Prince}=x})$, but let's demonstrate the fact with this causal LLM. First, we model $P(\\text{Kingdom}|\\text{Prince}=x)$, where \"x\" is:\n",
        "His courageous Prince takes command, leading the kingdom's army to victory in battle after battle\n",
        "To infer $P(\\text{Kingdom}|\\text{Prince}=x)$, we'll have to infer the latent confounder, King. I'll do this using the prince2king_model we trained. I'll use a probabilistic inference algorithm called importance resampling. I start by creating a proposal function (what Pyro calls a \"guide function\") that will generate samples of King and Kingdom, given Prince."
      ],
      "metadata": {
        "id": "c-VivIHDOau8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pyro.poutine as poutine\n",
        "from pyro.distributions import Categorical\n",
        "\n",
        "PRINCE_STORY = (    #A\n",
        "    \"His courageous Prince takes command, leading \"    #A\n",
        "    \"the kingdom's army to victory in battle after battle\")    #A\n",
        "cond_model = pyro.condition(    #A\n",
        "    causalLLM, {\"Prince\": encode(PRINCE_STORY)})    #A\n",
        "\n",
        "def proposal_given_prince():    #B\n",
        "    prince = encode(PRINCE_STORY)\n",
        "    king = pyro.sample(    #C\n",
        "        \"King\",    #C\n",
        "        TransformerModelDistribution(prince2king_model, prince)    #C\n",
        "    )    #C\n",
        "    king_and_prince = torch.cat([king, prince], dim=1)    #C\n",
        "    kingdom = pyro.sample(    #D\n",
        "        \"Kingdom\",    #D\n",
        "        TransformerModelDistribution(kingdom_model, king_and_prince)    #D\n",
        "    )    #D\n",
        "    vignette = (convert_to_text(king) +    #E\n",
        "        PRINCE_STORY +    #E\n",
        "        convert_to_text(kingdom))    #E\n",
        "    return vignette\n",
        "#A We condition the model on this a given value for the Prince variable.\n",
        "#B We'll use a proposal function to generates from our target distribution P(King, Kingdom|Prince=PRINCE_STORY).\n",
        "#C The proposal uses the prince2king_mode modell to infer values of King given Prince=PRINCE_STORY.\n",
        "#D Given the value of Prince, and inferred value of King, use the king_and_prince model to sample Kingdom.\n",
        "#E We concatenate the generated and conditioned upon tokens to return a generated vignette so we can inspect what is sampled."
      ],
      "metadata": {
        "id": "P739vZb8OhPV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Listing 13.21: Process sample for importance resampler\n",
        "\n",
        "Now we weigh each sample by the ratio of the probability of the sample under the conditioned model, over the probability of the sample under the proposal. Resampling the samples using these weights will generate samples from the target distribution. Pyro provides a utility for importance sampling, but because of the varying length of the generated sequences, it will be easier to implement importance sampling directly. First we write a function that returns a sample and its weight."
      ],
      "metadata": {
        "id": "1NOaJbnSQuFn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def process_sample(model, proposal):\n",
        "    sample_trace = poutine.trace(proposal).get_trace()    #A\n",
        "    king_text = convert_to_text(sample_trace.nodes['King']['value']) #A\n",
        "    kingdom_text = convert_to_text(\n",
        "        sample_trace.nodes['Kingdom']['value'])    #A\n",
        "    proposal_log_prob = sample_trace.log_prob_sum()    #B\n",
        "    replay = poutine.replay(model, trace=sample_trace)    #C\n",
        "    model_trace = poutine.trace(replay).get_trace()    #C\n",
        "    model_log_prob = model_trace.log_prob_sum()    #C\n",
        "    log_importance_weight = model_log_prob - proposal_log_prob    #D\n",
        "    sample = (king_text, kingdom_text, log_importance_weight)\n",
        "    return sample\n",
        "\n",
        "#A Extract a sample from the proposal.\n",
        "#B Calculate the total log probability of the sampled values of King and Kingdom.\n",
        "#C Next, calculate the total log probability of the sample values of King and Kingdom under the original model.\n",
        "#D Calculate the log importance weight.\n"
      ],
      "metadata": {
        "id": "RRkUHZuHCnyF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Listing 13.22: Importance resampling of $P(\\text{Kingdom}|\\text{Prince}=x)$"
      ],
      "metadata": {
        "id": "f6fo10LefJ18"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def do_importance_resampling(model, proposal, num_samples):    #A\n",
        "    original_samples = []\n",
        "    for _ in range(num_samples):\n",
        "        sample = process_sample(model, proposal)\n",
        "        original_samples.append(sample)\n",
        "    unique_samples = list(set(original_samples))    #B\n",
        "    log_importance_weights = torch.tensor(  #B\n",
        "        [sample[2] for sample in original_samples])  #B\n",
        "    resampling_dist = Categorical(logits=log_importance_weights)    #B\n",
        "    resampled_indices = resampling_dist.sample_n(num_samples)    #B\n",
        "    samples = pd.DataFrame(    #B\n",
        "        [unique_samples[i] for i in resampled_indices],     #B\n",
        "        columns=[\"King\", \"Kingdom\", \"log_importance_weight\"]    #B\n",
        "    )    #B\n",
        "    samples[\"Prince\"] = PRINCE_STORY\n",
        "    samples[\"Distribution\"] = \"observational\"\n",
        "    return samples[['King', 'Prince', \"Kingdom\", \"Distribution\"]]\n",
        "\n",
        "num_samples = 1000\n",
        "posterior_samples = do_importance_resampling(\n",
        "    cond_model, proposal_given_prince, num_samples)\n",
        "#A We use importance resampling as our inference procedure.\n",
        "#B Resample using the importance weights. Pass in the log weights to the \"logits\" argument.\n"
      ],
      "metadata": {
        "id": "quSmY9MLQmdB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5960b53c-5d36-4380-b6d2-d33a2a51e7ce"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/distributions/distribution.py:179: UserWarning: sample_n will be deprecated. Use .sample((n,)) instead\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Listing 13.23 Inferring $P(\\text{Kingdom}_{\\text{Prince}=x})$ using vanilla forward Monte Carlo sampling\n",
        "\n",
        "Next, we'll infer $P(\\text{Kingdom}_\\text{Prince}=x)$. Given our causal model in Pyro, we can use Pyro's do-operator to apply the intervention. We know that given the intervention on Prince, the edge from King to Prince is removed. So we don't need to use prince2king_model and can simply do ordinary forward generation from our intervention model."
      ],
      "metadata": {
        "id": "LuFi4e1TQzGU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "intervention_model = pyro.do(    #A\n",
        "    causalLLM, {\"Prince\": encode(PRINCE_STORY)})    #A\n",
        "intervention_samples = pd.DataFrame(    #B\n",
        "    [intervention_model() for _ in range(num_samples)],    #B\n",
        "    columns=[\"King\", \"Prince\", \"Kingdom\"]    #B\n",
        ")    #B\n",
        "intervention_samples[\"Distribution\"] = \"interventional\"    #B\n",
        "all_samples = pd.concat(    #B\n",
        "    [posterior_samples, intervention_samples],    #B\n",
        "    ignore_index=True    #B\n",
        ")    #B\n",
        "#A Forward sample from the interventional distribution.\n",
        "#B Label the samples, and combine them with the observational samples."
      ],
      "metadata": {
        "id": "DucANnbgRCcQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "86ceb19f-4f61-4cd3-9fea-4bf7eb08e042"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Listing 13.24 Calculate TF/IDF of generations from between $P(\\text{Kingdom}_{\\text{Prince}=x})$ and $P(\\text{Kingdom}|\\text{Prince}=x)$\n",
        "\n",
        "Next, let's visualize the difference in the distributions. Finally, we need a way to visualize sampled text from the interventional distribution and observational distributions. We can do so using TF-IDF (Term Frequency-Inverse Document Frequency), a numerical statistic that reflects how important a word is to a sample within the collection of samples, emphasizing words that are unique to specific samples."
      ],
      "metadata": {
        "id": "GM5vvthVRaCm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "kingdom_samples_url = (\n",
        "    \"https://raw.githubusercontent.com/altdeep/causalML/\"\n",
        "    \"master/book/chapter%2013/kingdom_samples.csv\")\n",
        "all_samples = pd.read_csv(kingdom_samples_url)\n",
        "\n",
        "observational_texts = all_samples[    #A\n",
        "    all_samples[\"Distribution\"] == \"observational\"][\"Kingdom\"]    #A\n",
        "interventional_texts = all_samples[all_samples[    #A\n",
        "    \"Distribution\"] == \"interventional\"][\"Kingdom\"]    #A\n",
        "\n",
        "vectorizer = TfidfVectorizer(stop_words='english')    #B\n",
        "X_obs = vectorizer.fit_transform(observational_texts)    #B\n",
        "X_int = vectorizer.transform(interventional_texts)    #B\n",
        "\n",
        "k = 10    #C\n",
        "feature_names = vectorizer.get_feature_names_out()    #C\n",
        "obs_indices = X_obs.sum(axis=0).argsort()[0, -k:][::-1]    #C\n",
        "int_indices = X_int.sum(axis=0).argsort()[0, -k:][::-1]    #C\n",
        "combined_indices = np.concatenate((obs_indices, int_indices))    #C\n",
        "combined_indices = np.unique(combined_indices)    #C\n",
        "#A Extract generated Kingdom vignettes from observational and interventional distributions.\n",
        "#B Compute the tf-idf values for generated Kingdom vignettes in each group.\n",
        "#C Get the top k=7 words by tf-idf for each set."
      ],
      "metadata": {
        "id": "6DkaDrhyRqFE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Listing 12.25: Visualize the difference between $P(\\text{Kingdom}_{\\text{Prince}=x})$ and $P(\\text{Kingdom}|\\text{Prince}=x)$\n",
        "\n",
        "Finally, we visualize the distributions.\n"
      ],
      "metadata": {
        "id": "GvicKlELRu1W"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "labels = [feature_names[i] for i in combined_indices]    #A\n",
        "labels, indices = np.unique(labels, return_index=True)    #A\n",
        "obs_values = np.array(X_obs.sum(axis=0))[0, combined_indices]    #A\n",
        "int_values = np.array(X_int.sum(axis=0))[0, combined_indices]    #A\n",
        "obs_values = [obs_values[0][i] for i in indices]    #A\n",
        "int_values = [int_values[0][i] for i in indices]    #A\n",
        "combined = list(zip(labels, obs_values, int_values))    #A\n",
        "sorted_combined = sorted(combined, key=lambda x: (-x[1], x[2]))    #A\n",
        "labels, obs_values, int_values = zip(*sorted_combined)    #A\n",
        "\n",
        "width = 0.35    #B\n",
        "x = np.arange(len(labels))    #B\n",
        "fig, ax = plt.subplots()    #B\n",
        "rects1 = ax.bar(x - width/2, obs_values, width,\n",
        "                label='Observational', alpha=0.7)    #B\n",
        "rects2 = ax.bar(x + width/2, int_values, width,\n",
        "                label='Interventional', alpha=0.7)    #B\n",
        "ax.set_xlabel('Words')    #B\n",
        "ax.set_ylabel('TF-IDF Values')    #B\n",
        "ax.set_title('Top Words in Generated Kingdom Vignettes by TF-IDF Value')    #B\n",
        "ax.set_xticks(x)    #B\n",
        "ax.set_xticklabels(labels)    #B\n",
        "ax.legend()    #B\n",
        "fig.tight_layout()    #B\n",
        "plt.xticks(rotation=45)    #B\n",
        "plt.show()    #B\n",
        "#A Prepare data for the bar plot.\n",
        "#B Produce the plot."
      ],
      "metadata": {
        "id": "jp0SgOXsRvBs"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
