{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "# Chapter 5 - Connecting Causality and Deep Learning\n",
    "\n",
    "The notebook is a code companion to chapter 5 of the book [Causal AI](https://www.manning.com/books/causal-ai) by [Robert Osazuwa Ness](https://www.linkedin.com/in/osazuwa/).\n",
    "\n",
    "<a href=\"https://colab.research.google.com/github/altdeep/causalML/blob/master/book/chapter%205/chapter_5_Connecting_Causality_and_Deep_Learning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "pgmpy allows us to fit conventional Bayesian networks on a causal DAG. However, with modern deep probabilistic machine learning frameworks like pyro, we can build more nuanced and powerful causal models.  In this tutorial, we fit a variational autoencoder on a causal DAG that represents a dataset that mixes handwritten MNIST digits and typed T-MNIST images. \n",
    "\n",
    "![TMNIST-MNIST](images/MNIST-TMNIST.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "a_gEnU8rWN3e",
    "outputId": "ce30baaf-b55a-46f5-8e07-7239a1b98736"
   },
   "outputs": [],
   "source": [
    "#!pip install pyro-ppl==1.8.4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ictI77JTWF5E",
    "outputId": "a59fa723-2bdb-4360-db43-6a2c0f4a0a34"
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'torchvision'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 15\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfunctional\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mF\u001b[39;00m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m random_split\n\u001b[0;32m---> 15\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorchvision\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m transforms\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'torchvision'"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import pyro\n",
    "import pyro.distributions as dist\n",
    "from pyro.infer import SVI, Trace_ELBO\n",
    "from pyro.optim import Adam\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import random_split\n",
    "from torchvision import transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lfWTRtPoWF5H"
   },
   "outputs": [],
   "source": [
    "USE_CUDA = False\n",
    "DEVICE_TYPE = torch.device(\"cuda\" if USE_CUDA else \"cpu\")\n",
    "BATCH_SIZE = 256\n",
    "LEARNING_RATE = 1.0e-3\n",
    "NUM_EPOCHS = 2500\n",
    "TEST_FREQUENCY = 10\n",
    "pyro.distributions.enable_validation(False)\n",
    "REINIT_PARAMS = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we download the data and combine it into a Dataset object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_GoP_qMKWF5I"
   },
   "outputs": [],
   "source": [
    "class CombinedDataset(Dataset):\n",
    "    def __init__(self, csv_file):\n",
    "        self.dataset = pd.read_csv(csv_file)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        images = self.dataset.iloc[idx, 3:]\n",
    "        images = np.array(images, dtype='float32')/255.\n",
    "        images = images.reshape(28, 28)\n",
    "        transform = transforms.ToTensor()\n",
    "        images = transform(images)\n",
    "        digits = self.dataset.iloc[idx, 2]\n",
    "        digits = np.array([digits], dtype='int')\n",
    "        is_handwritten = self.dataset.iloc[idx, 1]\n",
    "        is_handwritten = np.array([is_handwritten], dtype='float32')\n",
    "        return images, digits, is_handwritten\n",
    "\n",
    "def setup_dataloaders(batch_size=64, use_cuda=USE_CUDA):\n",
    "    combined_dataset = CombinedDataset(\n",
    "        \"https://raw.githubusercontent.com/altdeep/causalML/master/datasets/combined_mnist_tmnist_data.csv\"\n",
    "    )\n",
    "    n = len(combined_dataset)\n",
    "    train_size = int(0.8 * n)\n",
    "    test_size = n - train_size\n",
    "    train_dataset, test_dataset = random_split(\n",
    "        combined_dataset,\n",
    "        [train_size, test_size],\n",
    "        generator=torch.Generator().manual_seed(42)\n",
    "    )\n",
    "    kwargs = {'num_workers': 1, 'pin_memory': use_cuda}\n",
    "    train_loader = DataLoader(\n",
    "        train_dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=True,\n",
    "        **kwargs\n",
    "    )\n",
    "    test_loader = DataLoader(\n",
    "        test_dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=True,\n",
    "        **kwargs\n",
    "    )\n",
    "    return train_loader, test_loader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we specify and encoder and a decoder. The decoder maps the latent variable Z, a variable representing the value of the digit, and a binary variable representing whether the digit is handwritten The encoder takes an image, the digit, and whether the variable is handwritten, and infers the latent representation Z."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-vxH3pZfWF5K"
   },
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, z_dim, hidden_dim):\n",
    "        super().__init__()\n",
    "        img_dim = 28 * 28\n",
    "        digit_dim = 10\n",
    "        is_handwritten_dim = 1\n",
    "        self.fc1 = nn.Linear(z_dim + digit_dim + is_handwritten_dim, hidden_dim)\n",
    "        self.fc2 = nn.Linear(hidden_dim, img_dim)\n",
    "        self.softplus = nn.Softplus()\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "    \n",
    "    def forward(self, z, digit, is_handwritten):\n",
    "        input = torch.cat([z, digit, is_handwritten], dim=1)\n",
    "        hidden = self.softplus(self.fc1(input))\n",
    "        img_param = self.sigmoid(self.fc2(hidden))\n",
    "        return img_param\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, z_dim, hidden_dim):\n",
    "        super().__init__()\n",
    "        img_dim = 28 * 28\n",
    "        digit_dim = 10\n",
    "        is_handwritten_dim = 1\n",
    "        self.fc1 = nn.Linear(img_dim + digit_dim + is_handwritten_dim, hidden_dim)\n",
    "        self.fc21 = nn.Linear(hidden_dim, z_dim)\n",
    "        self.fc22 = nn.Linear(hidden_dim, z_dim)\n",
    "        self.softplus = nn.Softplus()\n",
    "    \n",
    "    def forward(self, img, digit, is_handwritten):\n",
    "        input = torch.cat([img, digit, is_handwritten], dim=1)\n",
    "        hidden = self.softplus(self.fc1(input))\n",
    "        z_loc = self.fc21(hidden)\n",
    "        z_scale = torch.exp(self.fc22(hidden))\n",
    "        return z_loc, z_scale"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we implement the variational autoencoder. The `model` method implements the causal model. First it samples the latent variable Z, the digit variable, and the is_handwritten variable. These are passed to the decoder, which generates the image.\n",
    "\n",
    "`training_model` extends `model` towards representing each image in the dataset. `training_guide` contains the encoder. The purpose of `training_guide` is to represent the approximating distribution during variational training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8PanaCBtWF5L"
   },
   "outputs": [],
   "source": [
    "class VAE(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        z_dim=50,\n",
    "        hidden_dim=400,\n",
    "        use_cuda=USE_CUDA,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.use_cuda = use_cuda\n",
    "        self.z_dim = z_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.setup_networks()\n",
    "    \n",
    "    def setup_networks(self):\n",
    "        self.encoder = Encoder(self.z_dim, self.hidden_dim)\n",
    "        self.decoder = Decoder(self.z_dim, self.hidden_dim)\n",
    "        if self.use_cuda:\n",
    "            self.cuda()\n",
    "    \n",
    "    def model(self, data_size=1):\n",
    "        pyro.module(\"decoder\", self.decoder)\n",
    "        options = dict(dtype=torch.float32, device=DEVICE_TYPE)\n",
    "        z_loc = torch.zeros(data_size, self.z_dim, **options)\n",
    "        z_scale = torch.ones(data_size, self.z_dim, **options)\n",
    "        z = pyro.sample(\"Z\", dist.Normal(z_loc, z_scale).to_event(1))\n",
    "        p_digit = torch.ones(data_size, 10, **options)/10\n",
    "        digit = pyro.sample(\n",
    "            \"digit\",\n",
    "            dist.OneHotCategorical(p_digit)\n",
    "        )\n",
    "        p_is_handwritten = torch.ones(data_size, 1, **options)/2\n",
    "        is_handwritten = pyro.sample(\n",
    "            \"is_handwritten\",\n",
    "            dist.Bernoulli(p_is_handwritten).to_event(1)\n",
    "        )\n",
    "        img_param = self.decoder(z, digit, is_handwritten)\n",
    "        img = pyro.sample(\"img\", dist.Bernoulli(img_param).to_event(1))\n",
    "        return img, digit, is_handwritten\n",
    "    \n",
    "    def training_model(self, img, digit, is_handwritten, batch_size):\n",
    "        model_conditioned_on_data = pyro.condition(\n",
    "            self.model,\n",
    "            data={\n",
    "                \"digit\": digit,\n",
    "                \"is_handwritten\": is_handwritten,\n",
    "                \"img\": img\n",
    "            }\n",
    "        )\n",
    "        with pyro.plate(\"data\", batch_size):\n",
    "            img, digit, is_handwritten = model_conditioned_on_data(batch_size)\n",
    "        return img, digit, is_handwritten\n",
    "    \n",
    "    def training_guide(self, img, digit, is_handwritten, batch_size):\n",
    "        pyro.module(\"encoder\", self.encoder)\n",
    "        options = dict(dtype=torch.float32, device=DEVICE_TYPE)\n",
    "        with pyro.plate(\"data\", batch_size):\n",
    "            z_loc, z_scale = self.encoder(img, digit, is_handwritten)\n",
    "            z = pyro.sample(\"Z\", dist.Normal(z_loc, z_scale).to_event(1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following utility functions helps us visualize progress during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "byhrjQwMWF5L"
   },
   "outputs": [],
   "source": [
    "def plot_image(img, title=None):\n",
    "    fig = plt.figure()\n",
    "    plt.imshow(img.cpu(), cmap='Greys_r', interpolation='nearest')\n",
    "    if title is not None:\n",
    "        plt.title(title)\n",
    "    plt.show()\n",
    "\n",
    "def reconstruct_img(vae, img, digit, is_handwritten, use_cuda=USE_CUDA):\n",
    "    img = img.reshape(-1, 28 * 28)\n",
    "    digit = F.one_hot(torch.tensor(digit), 10)\n",
    "    is_handwritten = torch.tensor(is_handwritten_rng).unsqueeze(0)\n",
    "    if use_cuda:\n",
    "      img, digit, is_handwritten = img.cuda(), digit.cuda(), is_handwritten.cuda()\n",
    "    z_loc, z_scale = vae.encoder(img, digit, is_handwritten)\n",
    "    z = dist.Normal(z_loc, z_scale).sample()\n",
    "    img_expectation = vae.decoder(z, digit, is_handwritten)\n",
    "    return img_expectation.squeeze().view(28, 28).detach()\n",
    "\n",
    "def compare_images(img1, img2):\n",
    "    fig = plt.figure()\n",
    "    ax0 = fig.add_subplot(121)\n",
    "    plt.imshow(img1.cpu(), cmap='Greys_r', interpolation='nearest')\n",
    "    plt.axis('off')\n",
    "    plt.title('original')\n",
    "    ax1 = fig.add_subplot(122)\n",
    "    plt.imshow(img2.cpu(), cmap='Greys_r', interpolation='nearest')\n",
    "    plt.axis('off')\n",
    "    plt.title('reconstruction')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These additional utility functions help us selected and reshape images, as well as generate new images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_random_example(loader):    \n",
    "    random_idx = np.random.randint(0, len(loader.dataset))\n",
    "    img, digit, is_handwritten = loader.dataset[random_idx]\n",
    "    return img.squeeze(), digit, is_handwritten\n",
    "\n",
    "def reshape_data(img, digit, is_handwritten):\n",
    "    digit = F.one_hot(digit, 10).squeeze()\n",
    "    img = img.reshape(-1, 28*28)\n",
    "    return img, digit, is_handwritten\n",
    "\n",
    "def generate_coded_data(vae, use_cuda=USE_CUDA):\n",
    "    z_loc = torch.zeros(1, vae.z_dim)\n",
    "    z_scale = torch.ones(1, vae.z_dim)\n",
    "    z = dist.Normal(z_loc, z_scale).to_event(1).sample()\n",
    "    p_digit = torch.ones(1, 10)/10\n",
    "    digit = dist.OneHotCategorical(p_digit).sample()\n",
    "    p_is_handwritten = torch.ones(1, 1)/2\n",
    "    is_handwritten = dist.Bernoulli(p_is_handwritten).sample()\n",
    "    if use_cuda:\n",
    "        z, digit, is_handwritten = z.cuda(), digit.cuda(), is_handwritten.cuda()\n",
    "    img = vae.decoder(z, digit, is_handwritten)\n",
    "    return img, digit, is_handwritten\n",
    "\n",
    "def generate_data(vae, use_cuda=USE_CUDA):\n",
    "    img, digit, is_handwritten = generate_coded_data(vae, use_cuda)\n",
    "    img = img.squeeze().view(28, 28).detach()\n",
    "    digit = torch.argmax(digit, 1)\n",
    "    is_handwritten = torch.argmax(is_handwritten, 1)\n",
    "    return img, digit, is_handwritten"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we run training. The training objective simultaneously trains the parameters of the encoder and the decoder. It focuses on minimizing reconstruction loss, meaning how much information is lost when an image encoded, and then decoded once again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "of1WjHsuWF5M",
    "outputId": "c4bf4f8b-ff60-46d9-b246-294f4b5f15ae"
   },
   "outputs": [],
   "source": [
    "pyro.clear_param_store()\n",
    "vae = VAE()\n",
    "\n",
    "train_loader, test_loader = setup_dataloaders(batch_size=BATCH_SIZE, use_cuda=USE_CUDA)\n",
    "train_size = len(train_loader.dataset)\n",
    "test_size = len(test_loader.dataset)\n",
    "\n",
    "svi_adam = pyro.optim.Adam({\"lr\": LEARNING_RATE})\n",
    "svi = SVI(vae.training_model, vae.training_guide, svi_adam, loss=Trace_ELBO())\n",
    "train_loss, test_loss = [], []\n",
    "\n",
    "for epoch in range(0, NUM_EPOCHS+1):\n",
    "    epoch_loss_train = 0\n",
    "    for batch_idx, (img, digit, is_handwritten) in enumerate(train_loader):\n",
    "        batch_size = img.shape[0]\n",
    "        if USE_CUDA:\n",
    "            img, digit, is_handwritten = img.cuda(), digit.cuda(), is_handwritten.cuda()\n",
    "        img, digit, is_handwritten = reshape_data(img, digit, is_handwritten)\n",
    "        epoch_loss_train += svi.step(img, digit, is_handwritten, batch_size)\n",
    "    epoch_loss_train = epoch_loss_train / train_size\n",
    "    print(\"Epoch: {} average training loss: {}\".format(epoch, epoch_loss_train))\n",
    "    train_loss.append(epoch_loss_train)\n",
    "    if epoch % TEST_FREQUENCY == 0:\n",
    "        epoch_loss_test = 0\n",
    "        for batch_idx, (img, digit, is_handwritten) in enumerate(test_loader):\n",
    "            batch_size = img.shape[0]\n",
    "            if USE_CUDA:\n",
    "                img, digit, is_handwritten = img.cuda(), digit.cuda(), is_handwritten.cuda()\n",
    "            img, digit, is_handwritten = reshape_data(img, digit, is_handwritten)\n",
    "            epoch_loss_test += svi.evaluate_loss(img, digit, is_handwritten, batch_size)\n",
    "        epoch_loss_test = epoch_loss_test/test_size\n",
    "        print(\"Epoch: {} average test loss: {}\".format(epoch, epoch_loss_test))\n",
    "        print(\"Comparing a random test image to its reconstruction:\")\n",
    "        img_rng, digit_rng, is_handwritten_rng = get_random_example(test_loader)\n",
    "        img_recon = reconstruct_img(vae, img_rng, digit_rng, is_handwritten_rng)\n",
    "        compare_images(img_rng, img_recon)\n",
    "        print(\"Generate a random image from the model:\")\n",
    "        img_gen, digit_gen, is_handwritten_gen = generate_data(vae)\n",
    "        plot_image(img_gen, \"Generated Image\")\n",
    "        print(\"Intended digit: \", int(digit_gen))\n",
    "        print(\"Intended as handwritten: \", bool(is_handwritten_gen == 1))\n",
    "#Plot training loss\n",
    "plt.plot(range(len(train_loss)), [-x for x in train_loss])\n",
    "plt.ylabel('Loss')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can continue to use `generate_data` to generate from the model once we've trained it. Finally, we can save the resulting model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 17
    },
    "id": "gxgJvMrecHJ5",
    "outputId": "c1569a1b-72ce-48f1-fdff-f64fc52c4c96"
   },
   "outputs": [],
   "source": [
    "\n",
    "#torch.save(vae.state_dict(), 'mnist_tmnist_weights_March11.pt')"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "include_colab_link": true,
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
